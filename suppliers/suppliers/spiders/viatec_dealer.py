"""
Spider –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–∏–ª–µ—Ä—Å–∫–∏—Ö —Ü–µ–Ω —Å viatec.ua (USD)
–¢—Ä–µ–±—É–µ—Ç –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º .env
–í—ã–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤: C:\\Users\\stalk\\Documents\\Prom\\prom_diler_import.csv
"""
import scrapy
import csv
from pathlib import Path
from urllib.parse import urljoin
import os
from dotenv import load_dotenv


class ViatecDealerSpider(scrapy.Spider):
    name = "viatec_dealer"
    allowed_domains = ["viatec.ua"]
    
    # URL –¥–ª—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ (–Ω—É–∂–Ω–æ —É—Ç–æ—á–Ω–∏—Ç—å —Ä–µ–∞–ª—å–Ω—ã–π URL)
    login_url = "https://viatec.ua/login"  # –ó–∞–º–µ–Ω–∏ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–π URL –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
    
    custom_settings = {
        "CONCURRENT_REQUESTS_PER_DOMAIN": 1,
        "DOWNLOAD_DELAY": 2,
        "COOKIES_ENABLED": True,
    }
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —É—á—ë—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ .env
        load_dotenv(Path(r"C:\FullStack\Scrapy\suppliers\.env"))
        self.email = os.getenv("VIATEC_EMAIL")
        self.password = os.getenv("VIATEC_PASSWORD")
        
        if not self.email or not self.password:
            raise ValueError("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —É—á—ë—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ .env")
        
        self.category_mapping = self._load_category_mapping()
        self.start_urls = list(self.category_mapping.keys())
    
    def _load_category_mapping(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–∞–ø–ø–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏–∑ CSV"""
        mapping = {}
        csv_path = Path(r"C:\FullStack\Scrapy\data\category_matching_viatec.csv")
        
        try:
            with open(csv_path, encoding="utf-8-sig") as f:
                reader = csv.DictReader(f, delimiter=";")
                for row in reader:
                    url = row["–õ–∏–Ω–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞"].strip().strip('"')
                    mapping[url] = {
                        "category_ru": row["–ö–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–∞ –º–æ–µ–º —Å–∞–π—Ç–µ_RU"],
                        "category_ua": row["–ö–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–∞ –º–æ–µ–º —Å–∞–π—Ç–µ_UA"],
                    }
            self.logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(mapping)} –∫–∞—Ç–µ–≥–æ—Ä–∏–π")
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {e}")
        
        return mapping
    
    def start_requests(self):
        """–ù–∞—á–∏–Ω–∞–µ–º —Å –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏"""
        self.logger.info("üîê –ù–∞—á–∏–Ω–∞–µ–º –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—é...")
        
        yield scrapy.Request(
            url=self.login_url,
            callback=self.login,
            dont_filter=True,
        )
    
    def login(self, response):
        """–ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ —Å–∞–π—Ç–µ"""
        # –ò—â–µ–º CSRF —Ç–æ–∫–µ–Ω, –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è
        csrf_token = response.css("input[name='csrf_token']::attr(value)").get()
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        formdata = {
            "email": self.email,
            "password": self.password,
        }
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å CSRF —Ç–æ–∫–µ–Ω
        if csrf_token:
            formdata["csrf_token"] = csrf_token
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º POST –∑–∞–ø—Ä–æ—Å
        yield scrapy.FormRequest.from_response(
            response,
            formdata=formdata,
            callback=self.after_login,
            dont_filter=True,
        )
    
    def after_login(self, response):
        """–ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –ø—Ä–∏–∑–Ω–∞–∫ —É—Å–ø–µ—à–Ω–æ–π –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        if "logout" in response.text.lower() or "–≤—ã—Ö–æ–¥" in response.text.lower():
            self.logger.info("‚úÖ –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞!")
            
            # –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            for url in self.start_urls:
                yield scrapy.Request(
                    url=url,
                    callback=self.parse_category,
                    meta={"category_url": url},
                    dont_filter=True,
                )
        else:
            self.logger.error("‚ùå –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å!")
            self.logger.debug(f"Response URL: {response.url}")
    
    def parse_category(self, response):
        """–ü–∞—Ä—Å–∏–º —Å–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        category_url = response.meta["category_url"]
        category_info = self.category_mapping.get(category_url, {})
        
        # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Ç–æ–≤–∞—Ä–æ–≤ (–ø—Ä–æ–≤–µ—Ä–µ–Ω–æ test_selectors.py)
        product_links = response.css("a[href*='/product/']::attr(href)").getall()
        
        if not product_links:
            self.logger.warning(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω—ã —Ç–æ–≤–∞—Ä—ã –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category_url}")
        
        for link in product_links:
            product_url = response.urljoin(link)
            yield scrapy.Request(
                url=product_url,
                callback=self.parse_product,
                meta={
                    "category_url": category_url,
                    "category_ru": category_info.get("category_ru", ""),
                    "category_ua": category_info.get("category_ua", ""),
                },
            )
        
        # –ü–∞–≥–∏–Ω–∞—Ü–∏—è (–Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤)
        next_page = (
            response.css("a.next-page::attr(href)").get() or
            response.css("a[rel='next']::attr(href)").get() or
            response.css("li.pagination-next a::attr(href)").get() or
            response.css("a.pagination__next::attr(href)").get() or
            response.css("a:contains('–î–∞–ª–µ–µ')::attr(href)").get() or
            response.css("a:contains('‚Üí')::attr(href)").get()
        )
        
        if next_page:
            self.logger.info(f"üìÑ –ù–∞–π–¥–µ–Ω–∞ —Å–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {next_page}")
            yield response.follow(
                next_page,
                callback=self.parse_category,
                meta={"category_url": category_url},
            )
        else:
            self.logger.info(f"‚úÖ –ü–∞–≥–∏–Ω–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category_url}")
    
    def parse_product(self, response):
        """–ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É —Ç–æ–≤–∞—Ä–∞ (–¥–∏–ª–µ—Ä—Å–∫–∞—è —Ü–µ–Ω–∞ –≤ USD)"""
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Ç–æ–≤–∞—Ä–∞
        name_ru = response.css("h1::text").get()
        
        name_ru = name_ru.strip() if name_ru else ""
        name_ua = name_ru
        
        # –ö–æ–¥ —Ç–æ–≤–∞—Ä–∞
        code = response.css("span.product-code::text").get()
        if not code:
            code = response.url.split("/")[-1]
        
        # –î–ò–õ–ï–†–°–ö–ê–Ø –¶–ï–ù–ê (—Å–µ–ª–µ–∫—Ç–æ—Ä –º–æ–∂–µ—Ç –æ—Ç–ª–∏—á–∞—Ç—å—Å—è)
        dealer_price = response.css("span.dealer-price::text").get()
        if not dealer_price:
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–µ–ª–µ–∫—Ç–æ—Ä –¥–ª—è –¥–∏–ª–µ—Ä—Å–∫–æ–π —Ü–µ–Ω—ã
            dealer_price = response.css("div.price-dealer::text").get()
        
        dealer_price = self._clean_price(dealer_price) if dealer_price else ""
        
        # –í–∞–ª—é—Ç–∞ –¥–ª—è –¥–∏–ª–µ—Ä–æ–≤ - USD
        currency = "USD"
        
        # –û–ø–∏—Å–∞–Ω–∏–µ
        description_ru = response.css("div.description::text").getall()
        description_ru = " ".join([d.strip() for d in description_ru if d.strip()])
        description_ua = description_ru
        
        # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        images = response.css("img.product-image::attr(src)").getall()
        image_url = response.urljoin(images[0]) if images else ""
        
        # –ù–∞–ª–∏—á–∏–µ
        availability = response.css("span.availability::text").get()
        availability = self._normalize_availability(availability)
        
        # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å
        manufacturer = response.css("span.manufacturer::text").get()
        manufacturer = manufacturer.strip() if manufacturer else ""
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        search_terms_ru = self._generate_search_terms(name_ru)
        search_terms_ua = self._generate_search_terms(name_ua)
        
        # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        specs = self._extract_specifications(response)
        
        yield {
            "–ö–æ–¥_—Ç–æ–≤–∞—Ä—É": code,
            "–ù–∞–∑–≤–∞_–ø–æ–∑–∏—Ü—ñ—ó": name_ru,
            "–ù–∞–∑–≤–∞_–ø–æ–∑–∏—Ü—ñ—ó_—É–∫—Ä": name_ua,
            "–ü–æ—à—É–∫–æ–≤—ñ_–∑–∞–ø–∏—Ç–∏": search_terms_ru,
            "–ü–æ—à—É–∫–æ–≤—ñ_–∑–∞–ø–∏—Ç–∏_—É–∫—Ä": search_terms_ua,
            "–û–ø–∏—Å": description_ru,
            "–û–ø–∏—Å_—É–∫—Ä": description_ua,
            "–¶—ñ–Ω–∞": dealer_price,
            "–í–∞–ª—é—Ç–∞": currency,
            "–û–¥–∏–Ω–∏—Ü—è_–≤–∏–º—ñ—Ä—É": "—à—Ç.",
            "–ü–æ—Å–∏–ª–∞–Ω–Ω—è_–∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è": image_url,
            "–ù–∞—è–≤–Ω—ñ—Å—Ç—å": availability,
            "–ù–∞–∑–≤–∞_–≥—Ä—É–ø–∏": response.meta.get("category_ru", ""),
            "–ù–∞–∑–≤–∞_–≥—Ä—É–ø–∏_—É–∫—Ä": response.meta.get("category_ua", ""),
            "–í–∏—Ä–æ–±–Ω–∏–∫": manufacturer,
            "–ö—Ä–∞—ó–Ω–∞_–≤–∏—Ä–æ–±–Ω–∏–∫": "",
            "price_type": "dealer",  # –ú–∞—Ä–∫–µ—Ä –¥–ª—è pipeline
            "–ü—Ä–æ–¥—É–∫—Ç_–Ω–∞_—Å–∞–π—Ç—ñ": response.url,
            **specs,
        }
    
    def _clean_price(self, price_str):
        """–û—á–∏—Å—Ç–∫–∞ —Ü–µ–Ω—ã –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        if not price_str:
            return ""
        
        # –£–¥–∞–ª—è–µ–º –≤—Å—ë –∫—Ä–æ–º–µ —Ü–∏—Ñ—Ä, —Ç–æ—á–∫–∏ –∏ –∑–∞–ø—è—Ç–æ–π
        price_str = price_str.replace(" ", "").replace("$", "").replace("USD", "")
        price_str = price_str.replace(",", ".")
        
        try:
            cleaned = "".join(c for c in price_str if c.isdigit() or c == ".")
            return str(float(cleaned)) if cleaned else ""
        except ValueError:
            return ""
    
    def _normalize_availability(self, availability):
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç—É—Å–∞ –Ω–∞–ª–∏—á–∏—è"""
        if not availability:
            return "–£—Ç–æ—á–Ω—è–π—Ç–µ"
        
        availability_lower = availability.lower()
        
        if any(word in availability_lower for word in ["—î –≤ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ", "–≤ –Ω–∞–ª–∏—á–∏–∏", "–µ—Å—Ç—å"]):
            return "–í –Ω–∞–ª–∏—á–∏–∏"
        elif any(word in availability_lower for word in ["–ø—ñ–¥ –∑–∞–º–æ–≤–ª–µ–Ω–Ω—è", "–ø–æ–¥ –∑–∞–∫–∞–∑"]):
            return "–ü–æ–¥ –∑–∞–∫–∞–∑"
        elif any(word in availability_lower for word in ["–Ω–µ–º–∞—î", "–Ω–µ—Ç"]):
            return "–ù–µ—Ç –≤ –Ω–∞–ª–∏—á–∏–∏"
        else:
            return "–£—Ç–æ—á–Ω—è–π—Ç–µ"
    
    def _generate_search_terms(self, product_name):
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤:
        '–ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∞, –°–ª–æ–≤–æ1, –°–ª–æ–≤–æ2, –°–ª–æ–≤–æ3, ...'
        """
        if not product_name:
            return ""
        
        # –ü–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ + —Å–ª–æ–≤–∞ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é
        words = product_name.replace(",", " ").split()
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
        unique_words = []
        seen = set()
        for word in words:
            word_clean = word.strip().lower()
            if len(word_clean) > 2 and word_clean not in seen:
                unique_words.append(word)
                seen.add(word_clean)
        
        # –§–æ—Ä–º–∞—Ç: "–ü–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ, –°–ª–æ–≤–æ1, –°–ª–æ–≤–æ2, ..."
        search_terms = f"{product_name}, {', '.join(unique_words)}"
        
        return search_terms
    
    def _extract_specifications(self, response):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —Ç–æ–≤–∞—Ä–∞"""
        specs = {}
        
        # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ (–∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥ viatec.ua)
        spec_rows = response.css("table.specifications tr")
        
        for i, row in enumerate(spec_rows[:30], 1):  # –ú–∞–∫—Å–∏–º—É–º 30 —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
            name = row.css("td:first-child::text").get()
            value = row.css("td:last-child::text").get()
            
            if name and value:
                specs[f"–ù–∞–∑–≤–∞_–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏_{i}"] = name.strip()
                specs[f"–ó–Ω–∞—á–µ–Ω–Ω—è_–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏_{i}"] = value.strip()
                specs[f"–û–¥–∏–Ω–∏—Ü—è_–≤–∏–º—ñ—Ä—É_–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏_{i}"] = ""
        
        return specs
