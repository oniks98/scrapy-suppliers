"""
Spider –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É —Ä–æ–∑–¥—Ä—ñ–±–Ω–∏—Ö —Ü—ñ–Ω –∑ secur.ua (UAH)
–í–∏–≥—Ä—É–∂–∞—î –¥–∞–Ω—ñ –≤: output/secur_retail.csv

–í–ò–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–∏–±—Ä–∞–Ω–æ wait_for_selector —â–æ –≤–∏–∫–ª–∏–∫–∞–≤ timeout
"""
import scrapy
import csv
import re
from pathlib import Path
from scrapy_playwright.page import PageMethod
from suppliers.spiders.base import BaseRetailSpider


class SecurRetailSpider(BaseRetailSpider):
    name = "secur_retail"
    supplier_id = "secur"
    output_filename = "secur_retail.csv"
    allowed_domains = ["secur.ua"]
    
    custom_settings = {
        "ITEM_PIPELINES": {
            "suppliers.pipelines.SuppliersPipeline": 300,
        },
        "DOWNLOAD_HANDLERS": {
            "http": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
            "https": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
        },
        "TWISTED_REACTOR": "twisted.internet.asyncioreactor.AsyncioSelectorReactor",
        "CONCURRENT_REQUESTS": 1,
        "DOWNLOAD_DELAY": 2,
    }
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.category_mapping = self._load_category_mapping()
        self.category_urls = list(self.category_mapping.keys())
        self.current_category_index = 0
        self.products_from_pagination = []
    
    def _load_category_mapping(self):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –º–∞–ø–ø—ñ–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä—ñ–π –∑ CSV"""
        mapping = {}
        csv_path = Path(r"C:\FullStack\Scrapy\data\secur\secur_category_retail.csv")
        
        try:
            with open(csv_path, encoding="utf-8-sig") as f:
                reader = csv.DictReader(f, delimiter=";")
                for row in reader:
                    url = row["–õ–∏–Ω–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞"].strip().strip('"')
                    
                    if not url or not url.startswith("http"):
                        continue
                    
                    mapping[url] = {
                        "category_ru": row["–ö–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–∞ –º–æ–µ–º —Å–∞–π—Ç–µ_RU"],
                        "category_ua": row["–ö–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–∞ –º–æ–µ–º —Å–∞–π—Ç–µ_UA"],
                        "group_number": row.get("–ù–æ–º–µ—Ä_–≥—Ä—É–ø–∏", ""),
                        "subdivision_id": row.get("–Ü–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ç–æ—Ä_–ø—ñ–¥—Ä–æ–∑–¥—ñ–ª—É", ""),
                        "subdivision_link": row.get("–ü–æ—Å–∏–ª–∞–Ω–Ω—è_–ø—ñ–¥—Ä–æ–∑–¥—ñ–ª—É", ""),
                    }
            self.logger.info(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(mapping)} –∫–∞—Ç–µ–≥–æ—Ä—ñ–π")
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–π: {e}")
        
        return mapping
    
    def start_requests(self):
        """–°—Ç–∞—Ä—Ç—É—î–º–æ –∑ –ø–µ—Ä—à–æ—ó –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó"""
        if self.category_urls:
            first_category_url = self.category_urls[0]
            self.logger.info(f"üöÄ –°–¢–ê–†–¢ –ü–ê–†–°–ò–ù–ì–£. –ü–µ—Ä—à–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è [1/{len(self.category_urls)}]: {first_category_url}")
            yield scrapy.Request(
                url=first_category_url,
                callback=self.parse_category,
                meta={
                    "category_url": first_category_url,
                    "category_index": 0,
                    "page_number": 1,
                    "playwright": True,
                },
                dont_filter=True,
                errback=self.errback_httpbin,
            )
    
    def errback_httpbin(self, failure):
        """–û–±—Ä–æ–±–∫–∞ –ø–æ–º–∏–ª–æ–∫"""
        self.logger.error(f"‚ùå ERRBACK: {failure.value}")
        self.logger.error(f"   URL: {failure.request.url}")
        
        # –û—Ç—Ä–∏–º—É—î–º–æ remaining_products –∑ meta
        remaining = failure.request.meta.get("remaining_products", [])
        category_index = failure.request.meta.get("category_index", 0)
        
        # –Ø–∫—â–æ —î —â–µ —Ç–æ–≤–∞—Ä–∏ - –æ–±—Ä–æ–±–ª—è—î–º–æ —ó—Ö
        if remaining:
            self.logger.info(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ —Ç–æ–≤–∞—Ä –∑ –ø–æ–º–∏–ª–∫–æ—é, –æ–±—Ä–æ–±–ª—è—î–º–æ –Ω–∞—Å—Ç—É–ø–Ω–∏–π ({len(remaining)} –∑–∞–ª–∏—à–∏–ª–æ—Å—å)")
            request_to_yield = self._process_next_item(remaining, category_index)
            if request_to_yield:
                yield request_to_yield
        else:
            # –Ü–Ω–∞–∫—à–µ –ø–µ—Ä–µ—Ö–æ–¥–∏–º–æ –¥–æ –Ω–∞—Å—Ç—É–ø–Ω–æ—ó –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó
            next_cat = self._start_next_category(category_index)
            if next_cat:
                yield next_cat
    
    def parse_category(self, response):
        """–ü–∞—Ä—Å–∏–º–æ —Å–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä—ñ–≤ —É –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó"""
        category_url = response.meta["category_url"]
        category_index = response.meta["category_index"]
        page_number = response.meta.get("page_number", 1)
        category_info = self.category_mapping.get(category_url, {})
        
        self.logger.info(f"üìÇ –û–±—Ä–æ–±–ª—è—é –∫–∞—Ç–µ–≥–æ—Ä—ñ—é [{category_index + 1}/{len(self.category_urls)}] —Å—Ç–æ—Ä—ñ–Ω–∫–∞ {page_number}")
        
        product_links = response.css('div.productsCardsSlider a::attr(href)').getall()
        
        if not product_links:
            self.logger.warning(f"‚ö†Ô∏è –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä—ñ–≤ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ: {response.url}")
        else:
            self.logger.info(f"üì¶ –ó–Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä—ñ–≤ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ: {len(product_links)}")
            for link in product_links:
                product_url = response.urljoin(link)
                
                if product_url not in self.processed_products:
                    self.products_from_pagination.append({
                        "url": product_url,
                        "meta": {
                            "category_url": category_url,
                            "category_ru": category_info.get("category_ru", ""),
                            "category_ua": category_info.get("category_ua", ""),
                            "group_number": category_info.get("group_number", ""),
                            "subdivision_id": category_info.get("subdivision_id", ""),
                            "subdivision_link": category_info.get("subdivision_link", ""),
                        },
                    })
                    self.processed_products.add(product_url)
        
        next_page = response.css('a.next-button::attr(href)').get()
        
        if next_page:
            next_page_url = response.urljoin(next_page)
            self.logger.info(f"üìÑ –ü–∞–≥—ñ–Ω–∞—Ü—ñ—è: —Å—Ç–æ—Ä—ñ–Ω–∫–∞ {page_number + 1}")
            yield scrapy.Request(
                url=next_page_url,
                callback=self.parse_category,
                meta={
                    "category_url": category_url,
                    "category_index": category_index,
                    "page_number": page_number + 1,
                    "playwright": True,
                },
                dont_filter=True,
                errback=self.errback_httpbin,
            )
        else:
            self.logger.info(f"‚úÖ –ü–ê–ì–Ü–ù–ê–¶–Ü–Ø –ó–ê–í–ï–†–®–ï–ù–ê [{category_index + 1}/{len(self.category_urls)}]: –Ω–∞–∫–æ–ø–∏—á–µ–Ω–æ {len(self.products_from_pagination)} —Ç–æ–≤–∞—Ä—ñ–≤")
            
            if self.products_from_pagination:
                product_data = self.products_from_pagination.pop(0)
                product_data["meta"]["remaining_products"] = list(self.products_from_pagination)
                product_data["meta"]["category_index"] = category_index
                
                self.logger.info(f"üîó –ó–ê–ü–£–°–ö –ª–∞–Ω—Ü—é–≥–∞ –ø—Ä–æ–¥—É–∫—Ç—ñ–≤. –ü–µ—Ä—à–∏–π: {product_data['url']}. –ó–∞–ª–∏—à–∏–ª–æ—Å—å: {len(self.products_from_pagination)}")
                
                # –ü—Ä–æ—Å—Ç–æ —á–µ–∫–∞—î–º–æ 3 —Å–µ–∫—É–Ω–¥–∏ - –±–µ–∑ wait_for_selector
                yield scrapy.Request(
                    url=product_data["url"],
                    callback=self.parse_product_ua,
                    meta={
                        **product_data["meta"],
                        "playwright": True,
                        "playwright_page_methods": [
                            PageMethod("wait_for_timeout", 2000),  # –ß–µ–∫–∞—î–º–æ 2 —Å–µ–∫—É–Ω–¥–∏ –¥–ª—è Vue.js
                        ],
                    },
                    dont_filter=True,
                    errback=self.errback_httpbin,
                )
            else:
                self.logger.warning(f"‚ö†Ô∏è –£ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó {category_url} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä—ñ–≤. –ü–µ—Ä–µ—Ö–æ–¥–∂—É –¥–æ –Ω–∞—Å—Ç—É–ø–Ω–æ—ó.")
                next_cat = self._start_next_category(category_index)
                if next_cat:
                    yield next_cat
    
    def parse_product_ua(self, response):
        """–ü–∞—Ä—Å–∏–º —É–∫—Ä–∞–∏–Ω—Å–∫—É—é –≤–µ—Ä—Å–∏—é —Ç–æ–≤–∞—Ä–∞"""
        self.logger.info(f"üá∫üá¶ UA: {response.url}")
        
        name_ua = response.css('h1.title::text').get()
        price_raw = response.css('div.currentPrice span.bold::text').get()
        image_url = response.css('div.productsCardsSlider a img::attr(src)').get()
        product_code = response.css('div.productsCardsCode span::text').get()
        
        availability_raw = response.css('div.statusWrap::text').get()
        if availability_raw:
            availability_raw = availability_raw.strip()
        else:
            availability_raw = "–í –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ"
        
        description_ua = response.css('div.content.descr div.item').get()
        if description_ua:
            description_ua = self._clean_html_description(description_ua)
        else:
            description_ua = ""
        
        specs_list = self._parse_specifications(response)
        
        self.logger.info(f"üìä UA: –ó–Ω–∞–π–¥–µ–Ω–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫: {len(specs_list)}")
        
        meta = response.meta.copy()
        meta.update({
            "name_ua": name_ua.strip() if name_ua else "",
            "price_raw": price_raw,
            "image_url": image_url,
            "product_code": product_code.strip() if product_code else "",
            "availability_raw": availability_raw,
            "description_ua": description_ua,
            "specs_list": specs_list,
        })
        
        ru_url = response.url.replace("secur.ua/", "secur.ua/ru/")
        
        yield scrapy.Request(
            url=ru_url,
            callback=self.parse_product_ru,
            meta={
                **meta,
                "playwright": True,
                "playwright_page_methods": [
                    PageMethod("wait_for_timeout", 2000),
                ],
            },
            dont_filter=True,
            errback=self.errback_httpbin,
        )
    
    def parse_product_ru(self, response):
        """–ü–∞—Ä—Å–∏–º —Ä—É—Å—Å–∫—É—é –≤–µ—Ä—Å–∏—é —Ç–æ–≤–∞—Ä–∞"""
        self.logger.info(f"üá∑üá∫ RU: {response.url}")
        
        name_ru = response.css('h1.title::text').get()
        brand = response.xpath("//div[@class='subtitle' and text()='–ë—Ä–µ–Ω–¥']/../div[@class='inner']//p/text()").get()
        
        description_ru = response.css('div.content.descr div.item').get()
        if description_ru:
            description_ru = self._clean_html_description(description_ru)
        else:
            description_ru = ""
        
        name_ua = response.meta.get("name_ua", "")
        name_ru = name_ru.strip() if name_ru else name_ua
        price_raw = response.meta.get("price_raw", "")
        image_url = response.meta.get("image_url", "")
        product_code = response.meta.get("product_code", "")
        availability_raw = response.meta.get("availability_raw", "")
        description_ua = response.meta.get("description_ua", "")
        specs_list = response.meta.get("specs_list", [])
        
        price = self._clean_price(price_raw) if price_raw else ""
        image_url = response.urljoin(image_url) if image_url else ""
        brand = brand.strip() if brand else ""
        quantity = self._extract_quantity(availability_raw)
        
        search_terms_ru = self._generate_search_terms(name_ru)
        search_terms_ua = self._generate_search_terms(name_ua)
        
        self.logger.info(f"üìù –û–ø–∏—Å RU: {len(description_ru)} —Å–∏–º–≤–æ–ª—ñ–≤")
        self.logger.info(f"üìù –û–ø–∏—Å UA: {len(description_ua)} —Å–∏–º–≤–æ–ª—ñ–≤")
        
        item = {
            "–ö–æ–¥_—Ç–æ–≤–∞—Ä—É": product_code,
            "–ù–∞–∑–≤–∞_–ø–æ–∑–∏—Ü—ñ—ó": name_ru,
            "–ù–∞–∑–≤–∞_–ø–æ–∑–∏—Ü—ñ—ó_—É–∫—Ä": name_ua,
            "–ü–æ—à—É–∫–æ–≤—ñ_–∑–∞–ø–∏—Ç–∏": search_terms_ru,
            "–ü–æ—à—É–∫–æ–≤—ñ_–∑–∞–ø–∏—Ç–∏_—É–∫—Ä": search_terms_ua,
            "–û–ø–∏—Å": description_ru,
            "–û–ø–∏—Å_—É–∫—Ä": description_ua,
            "–¢–∏–ø_—Ç–æ–≤–∞—Ä—É": "r",
            "–¶—ñ–Ω–∞": price,
            "–í–∞–ª—é—Ç–∞": self.currency,
            "–û–¥–∏–Ω–∏—Ü—è_–≤–∏–º—ñ—Ä—É": "—à—Ç.",
            "–ü–æ—Å–∏–ª–∞–Ω–Ω—è_–∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è": image_url,
            "–ù–∞—è–≤–Ω—ñ—Å—Ç—å": availability_raw,
            "–ö—ñ–ª—å–∫—ñ—Å—Ç—å": quantity,
            "–ù–∞–∑–≤–∞_–≥—Ä—É–ø–∏": response.meta.get("category_ru", ""),
            "–ù–∞–∑–≤–∞_–≥—Ä—É–ø–∏_—É–∫—Ä": response.meta.get("category_ua", ""),
            "–ù–æ–º–µ—Ä_–≥—Ä—É–ø–∏": response.meta.get("group_number", ""),
            "–Ü–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ç–æ—Ä_–ø—ñ–¥—Ä–æ–∑–¥—ñ–ª—É": response.meta.get("subdivision_id", ""),
            "–ü–æ—Å–∏–ª–∞–Ω–Ω—è_–ø—ñ–¥—Ä–æ–∑–¥—ñ–ª—É": response.meta.get("subdivision_link", ""),
            "–í–∏—Ä–æ–±–Ω–∏–∫": brand,
            "–ö—Ä–∞—ó–Ω–∞_–≤–∏—Ä–æ–±–Ω–∏–∫": "",
            "price_type": self.price_type,
            "supplier_id": self.supplier_id,
            "output_file": self.output_filename,
            "–ü—Ä–æ–¥—É–∫—Ç_–Ω–∞_—Å–∞–π—Ç—ñ": response.url.replace("/ru/", "/"),
            "specifications_list": specs_list,
        }
        
        self.logger.info(f"‚úÖ YIELD: {item['–ù–∞–∑–≤–∞_–ø–æ–∑–∏—Ü—ñ—ó']} | –¶—ñ–Ω–∞: {item['–¶—ñ–Ω–∞']} | –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫: {len(specs_list)}")
        yield item
        
        # –û–±—Ä–æ–±–ª—è—î–º–æ –Ω–∞—Å—Ç—É–ø–Ω–∏–π —Ç–æ–≤–∞—Ä
        remaining = response.meta.get("remaining_products", [])
        category_index = response.meta.get("category_index", 0)
        
        request_to_yield = self._process_next_item(remaining, category_index)
        if request_to_yield:
            yield request_to_yield
    
    def _process_next_item(self, remaining, category_index):
        """–û–±—Ä–æ–±–ª—è—î –Ω–∞—Å—Ç—É–ø–Ω–∏–π —Ç–æ–≤–∞—Ä –∞–±–æ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç—å –¥–æ –Ω–∞—Å—Ç—É–ø–Ω–æ—ó –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó"""
        if remaining:
            next_data = remaining.pop(0)
            next_data["meta"]["remaining_products"] = list(remaining)
            next_data["meta"]["category_index"] = category_index
            
            self.logger.info(f"‚è≠Ô∏è –ù–∞—Å—Ç—É–ø–Ω–∏–π —Ç–æ–≤–∞—Ä ({len(remaining)} –∑–∞–ª–∏—à–∏–ª–æ—Å—å)")
            
            return scrapy.Request(
                url=next_data["url"],
                callback=self.parse_product_ua,
                meta={
                    **next_data["meta"],
                    "playwright": True,
                    "playwright_page_methods": [
                        PageMethod("wait_for_timeout", 2000),
                    ],
                },
                dont_filter=True,
                errback=self.errback_httpbin,
            )
        else:
            self.logger.info(f"‚úÖ –í–°–Ü –¢–û–í–ê–†–ò –ö–ê–¢–ï–ì–û–†–Ü–á –û–ë–†–û–ë–õ–ï–ù–Ü")
            return self._start_next_category(category_index)
    
    def _parse_specifications(self, response):
        """
        –ü–∞—Ä—Å–∏–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Ç–æ–≤–∞—Ä–∞
        –°—Ç—Ä—É–∫—Ç—É—Ä–∞ HTML: 
        <div class="item"><div class="subtitle">–ù–∞–∑–≤–∞</div><div class="inner"><div class="innerItem"><p>–ó–Ω–∞—á–µ–Ω–Ω—è</p></div></div></div>
        """
        specs_list = []
        
        # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –≤—Å—ñ div.item —è–∫—ñ –º—ñ—Å—Ç—è—Ç—å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        # –í–æ–Ω–∏ –º–æ–∂—É—Ç—å –±—É—Ç–∏ –≤ —Ä—ñ–∑–Ω–∏—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞—Ö, —Ç–æ–º—É —à—É–∫–∞—î–º–æ –≥–ª–æ–±–∞–ª—å–Ω–æ
        items = response.xpath('//div[@class="item"][.//div[@class="subtitle"]]')
        
        self.logger.info(f"üîç –ó–Ω–∞–π–¥–µ–Ω–æ {len(items)} –µ–ª–µ–º–µ–Ω—Ç—ñ–≤ div.item –∑ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏")
        
        for item in items:
            # –í–∏—Ç—è–≥—É—î–º–æ –Ω–∞–∑–≤—É —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
            characteristic = item.xpath('.//div[@class="subtitle"]/text()').get()
            
            if not characteristic:
                continue
            
            characteristic = characteristic.strip()
            
            # –í–∏—Ç—è–≥—É—î–º–æ –∑–Ω–∞—á–µ–Ω–Ω—è - –≤—Å—ñ —Ç–µ–∫—Å—Ç–æ–≤—ñ –≤—É–∑–ª–∏ –≤ div.inner
            value_texts = item.xpath('.//div[@class="inner"]//text()').getall()
            value = ' '.join(t.strip() for t in value_texts if t.strip())
            
            if value:
                value = value.replace('\u00a0', ' ').strip()
                specs_list.append({
                    "name": characteristic,
                    "unit": "",
                    "value": value,
                })
        
        return specs_list
    
    def _clean_html_description(self, html_content):
        """–û—á–∏—â–∞–µ–º HTML –æ–ø–∏—Å–∞–Ω–∏–µ, —Å–æ—Ö—Ä–∞–Ω—è—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ"""
        if not html_content:
            return ""
        
        from scrapy.selector import Selector
        sel = Selector(text=html_content)
        
        description_html = sel.css('div.item').get()
        
        if not description_html:
            return ""
        
        description_html = re.sub(r'^<div[^>]*>', '', description_html)
        description_html = re.sub(r'</div>$', '', description_html)
        description_html = re.sub(r'\s*style="[^"]*"', '', description_html)
        description_html = re.sub(r'>\s+<', '><', description_html)
        
        if len(description_html) > 10000:
            description_html = description_html[:10000] + '...</p>'
        
        return description_html.strip()
    
    def closed(self, reason):
        """–í–∏–∫–ª–∏–∫–∞—î—Ç—å—Å—è –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ñ –ø–∞—É–∫–∞"""
        self.logger.info(f"üéâ –ü–∞—É–∫ {self.name} –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –ü—Ä–∏—á–∏–Ω–∞: {reason}")
        
        if self.failed_products:
            self.logger.info("=" * 80)
            self.logger.info("üì¶ –°–ü–ò–°–û–ö –¢–û–í–ê–†–Ü–í –ó –ü–û–ú–ò–õ–ö–ê–ú–ò –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø")
            self.logger.info("=" * 80)
            for failed in self.failed_products:
                self.logger.error(f"- –¢–æ–≤–∞—Ä: {failed['product_name']} | URL: {failed['url']} | –ü—Ä–∏—á–∏–Ω–∞: {failed['reason']}")
            self.logger.info("=" * 80)
        else:
            self.logger.info("‚úÖ –¢–æ–≤–∞—Ä—ñ–≤ –∑ –ø–æ–º–∏–ª–∫–∞–º–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
        
        # –ó–≤—É–∫–æ–≤–∏–π —Å–∏–≥–Ω–∞–ª (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ, –ø—Ä–∞—Ü—é—î —Ç—ñ–ª—å–∫–∏ –Ω–∞ Windows)
        try:
            import winsound
            for _ in range(3):
                winsound.Beep(1000, 300)
            self.logger.info("üîî –ó–≤—É–∫–æ–≤–∏–π —Å–∏–≥–Ω–∞–ª –≤—ñ–¥—Ç–≤–æ—Ä–µ–Ω–æ!")
        except Exception as e:
            self.logger.debug(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –≤—ñ–¥—Ç–≤–æ—Ä–∏—Ç–∏ –∑–≤—É–∫: {e}")
    
    def _start_next_category(self, current_category_index):
        """–ó–∞–ø—É—Å–∫ —Å–ª–µ–¥—É—é—â–µ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        next_category_index = current_category_index + 1
        if next_category_index < len(self.category_urls):
            next_category_url = self.category_urls[next_category_index]
            self.logger.info(f"üöÄ –ù–ê–°–¢–£–ü–ù–ê –ö–ê–¢–ï–ì–û–†–Ü–Ø [{next_category_index + 1}/{len(self.category_urls)}]")
            self.products_from_pagination = []
            return scrapy.Request(
                url=next_category_url,
                callback=self.parse_category,
                meta={
                    "category_url": next_category_url,
                    "category_index": next_category_index,
                    "page_number": 1,
                    "playwright": True,
                },
                dont_filter=True,
                errback=self.errback_httpbin,
            )
        else:
            self.logger.info(f"üéâ –í–°–Ü –ö–ê–¢–ï–ì–û–†–Ü–á –û–ë–†–û–ë–õ–ï–ù–Ü üéâ")
            return None
