"""
Spider –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ä–æ–∑–Ω–∏—á–Ω—ã—Ö —Ü–µ–Ω —Å viatec.ua (UAH)
–í—ã–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤: C:\FullStack\Scrapy\output\prom_import.csv

‚ö†Ô∏è –í–ê–ñ–ù–û: –ü–∞—É–∫ —Å–æ–∑–¥–∞—ë—Ç –¢–û–õ–¨–ö–û —Ñ–∞–π–ª —Ä–æ–∑–Ω–∏—Ü—ã (prom_import.csv)
–§–∞–π–ª –¥–∏–ª–µ—Ä–∞ –ù–ï —Å–æ–∑–¥–∞—ë—Ç—Å—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ —ç—Ç–æ–≥–æ –ø–∞—É–∫–∞

–ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê: –∫–∞—Ç–µ–≥–æ—Ä–∏—è ‚Üí –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ ‚Üí —Å–ª–µ–¥—É—é—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è
–•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö–ò: –ø–∞—Ä—Å—è—Ç—Å—è –Ω–∞ –£–ö–†–ê–ò–ù–°–ö–û–ú (UA) —è–∑—ã–∫–µ
"""
import scrapy
import csv
import re
from pathlib import Path
from urllib.parse import urljoin
from scrapy import Selector
import winsound


class ViatecRetailSpider(scrapy.Spider):
    name = "viatec_retail"
    allowed_domains = ["viatec.ua"]
    
    custom_settings = {
        "CONCURRENT_REQUESTS": 8,  # Allow more concurrent requests
        "CONCURRENT_REQUESTS_PER_DOMAIN": 8, # Allow more concurrent requests per domain
        "AUTOTHROTTLE_ENABLED": True, # Enable AutoThrottle
        "AUTOTHROTTLE_START_DELAY": 1, # Initial delay before AutoThrottle kicks in
        "AUTOTHROTTLE_MAX_DELAY": 60, # Maximum delay AutoThrottle can set
        "AUTOTHROTTLE_TARGET_CONCURRENCY": 2.0, # Aim for 2 concurrent requests per second
        # "DOWNLOAD_DELAY": 2, # Removed, as AutoThrottle manages delays
    }
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.category_mapping = self._load_category_mapping()
        self.category_urls = list(self.category_mapping.keys())
        self.current_category_index = 0
        self.products_from_pagination = []  # –ù–∞–∫–∞–ø–ª–∏–≤–∞–µ–º —Ç–æ–≤–∞—Ä—ã —Å–æ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
        self.processed_products = set()  # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã (–ø–æ original_url)
    
    def _load_category_mapping(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–∞–ø–ø–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏–∑ CSV"""
        mapping = {}
        csv_path = Path(r"C:\FullStack\Scrapy\data\category_matching_retail_viatec.csv")
        
        try:
            with open(csv_path, encoding="utf-8-sig") as f:
                reader = csv.DictReader(f, delimiter=";")
                for row in reader:
                    url = row["–õ–∏–Ω–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞"].strip().strip('"')
                    
                    if not url or url == "" or not url.startswith("http"):
                        continue
                    
                    mapping[url] = {
                        "category_ru": row["–ö–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–∞ –º–æ–µ–º —Å–∞–π—Ç–µ_RU"],
                        "category_ua": row["–ö–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–∞ –º–æ–µ–º —Å–∞–π—Ç–µ_UA"],
                        "group_number": row.get("–ù–æ–º–µ—Ä_–≥—Ä—É–ø–∏", ""),
                        "subdivision_id": row.get("–Ü–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ç–æ—Ä_–ø—ñ–¥—Ä–æ–∑–¥—ñ–ª—É", ""),
                        "subdivision_link": row.get("–ü–æ—Å–∏–ª–∞–Ω–Ω—è_–ø—ñ–¥—Ä–æ–∑–¥—ñ–ª—É", ""),
                    }
            self.logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(mapping)} –∫–∞—Ç–µ–≥–æ—Ä–∏–π")
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {e}")
        
        return mapping
    
    def start_requests(self):
        """–°—Ç–∞—Ä—Ç—É–µ–º —Å –ø–µ—Ä–≤–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        if self.category_urls:
            first_category_url = self.category_urls[0]
            self.logger.info(f"üöÄ –°–¢–ê–†–¢ –ü–ê–†–°–ò–ù–ì–ê. –ü–µ—Ä–≤–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è [1/{len(self.category_urls)}]: {first_category_url}")
            yield scrapy.Request(
                url=first_category_url,
                callback=self.parse_category,
                meta={
                    "category_url": first_category_url,
                    "category_index": 0,
                    "page_number": 1,
                },
                dont_filter=True,
            )

    def parse_category(self, response):
        """–ü–∞—Ä—Å–∏–º —Å–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–∞–≥–∏–Ω–∞—Ü–∏–∏"""
        category_url = response.meta["category_url"]
        category_index = response.meta["category_index"]
        page_number = response.meta.get("page_number", 1)
        category_info = self.category_mapping.get(category_url, {})

        self.logger.info(f"üìÇ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é [{category_index + 1}/{len(self.category_urls)}] —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_number}: {category_url}")

        product_links = response.css("a[href*='/product/']::attr(href)").getall()

        if not product_links:
            self.logger.warning(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω—ã —Ç–æ–≤–∞—Ä—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {response.url}")
        else:
            self.logger.info(f"üì¶ –ù–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {len(product_links)}")
            for link in product_links:
                product_url = response.urljoin(link)
                # üõ°Ô∏è –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º URL (—É–±–∏—Ä–∞–µ–º /ru/ –µ—Å–ª–∏ –µ—Å—Ç—å)
                normalized_url = product_url.replace("/ru/", "/")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –¥–æ–±–∞–≤–ª—è–ª–∏ –ª–∏ –º—ã —É–∂–µ —ç—Ç–æ—Ç —Ç–æ–≤–∞—Ä
                if normalized_url not in self.processed_products:
                    self.products_from_pagination.append({
                        "url": normalized_url,
                        "meta": {
                            "category_url": category_url,
                            "category_ru": category_info.get("category_ru", ""),
                            "category_ua": category_info.get("category_ua", ""),
                            "group_number": category_info.get("group_number", ""),
                            "subdivision_id": category_info.get("subdivision_id", ""),
                            "subdivision_link": category_info.get("subdivision_link", ""),
                        },
                    })
                    self.processed_products.add(normalized_url)

        next_page_link = response.css("a.paggination__next::attr(href)").get()
        if not next_page_link:
            all_pages = response.css("a.paggination__page::attr(href)").getall()
            active_page_nodes = response.css("a.paggination__page--active")
            if all_pages and active_page_nodes:
                try:
                    # –ò—â–µ–º –∏–Ω–¥–µ–∫—Å –∞–∫—Ç–∏–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    active_page_text = active_page_nodes[0].css("::text").get()
                    all_page_texts = [a.css("::text").get() for a in response.css("a.paggination__page")]
                    current_idx = all_page_texts.index(active_page_text)
                    
                    if current_idx >= 0 and current_idx + 1 < len(all_pages):
                        next_page_link = all_pages[current_idx + 1]
                except (ValueError, IndexError):
                    pass # –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É, —Å—á–∏—Ç–∞–µ–º —á—Ç–æ —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è

        if next_page_link:
            self.logger.info(f"üìÑ –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ ({page_number + 1}): {next_page_link}")
            yield response.follow(
                next_page_link,
                callback=self.parse_category,
                meta={
                    "category_url": category_url,
                    "category_index": category_index,
                    "page_number": page_number + 1,
                },
                dont_filter=True,
            )
        else:
            self.logger.info(f"‚úÖ –ü–ê–ì–ò–ù–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê [{category_index + 1}/{len(self.category_urls)}]: –Ω–∞–∫–æ–ø–ª–µ–Ω–æ {len(self.products_from_pagination)} —Ç–æ–≤–∞—Ä–æ–≤")
            
            if self.products_from_pagination:
                # –ó–∞–ø—É—Å–∫–∞–µ–º —Ü–µ–ø–æ—á–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–¥—É–∫—Ç–æ–≤
                product_data = self.products_from_pagination.pop(0)
                
                product_data["meta"]["remaining_products"] = self.products_from_pagination
                product_data["meta"]["category_index"] = category_index
                
                self.logger.info(f"üîó –ó–ê–ü–£–°–ö —Ü–µ–ø–æ—á–∫–∏ –ø—Ä–æ–¥—É–∫—Ç–æ–≤. –ü–µ—Ä–≤—ã–π: {product_data['url']}. –û—Å—Ç–∞–ª–æ—Å—å: {len(self.products_from_pagination)}")
                
                yield scrapy.Request(
                    url=product_data["url"],
                    callback=self.parse_product,
                    meta=product_data["meta"],
                    dont_filter=True,
                )
            else:
                # –ï—Å–ª–∏ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–µ—Ç —Ç–æ–≤–∞—Ä–æ–≤, –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–π
                self.logger.warning(f"‚ö†Ô∏è –í –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {category_url} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤. –ü–µ—Ä–µ—Ö–æ–∂—É –∫ —Å–ª–µ–¥—É—é—â–µ–π.")
                yield self._start_next_category(category_index)

            # –û—á–∏—â–∞–µ–º –±—É—Ñ–µ—Ä
            self.products_from_pagination = []

    def parse_product(self, response):
        """–ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É —Ç–æ–≤–∞—Ä–∞ (—É–∫—Ä–∞–∏–Ω—Å–∫–∞—è –≤–µ—Ä—Å–∏—è) - –ù–ê–ó–í–ê–ù–ò–ï, –û–ü–ò–°–ê–ù–ò–ï, –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö–ò"""
        self.logger.info(f"üîó –ü–∞—Ä—Å–∏–º —Ç–æ–≤–∞—Ä (UA): {response.url}")
        
        name_ua = response.css("h1::text").get()
        name_ua = name_ua.strip() if name_ua else ""
        
        description_ua = self._extract_description_with_br(response)
        
        # ‚ö†Ô∏è –í–ê–ñ–ù–û: –ü–∞—Ä—Å–∏–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Å –£–ö–†–ê–ò–ù–°–ö–û–ô –≤–µ—Ä—Å–∏–∏
        specs_list_ua = self._extract_specifications(response)
        
        self.logger.info(f"üìê –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ (UA) –Ω–∞–π–¥–µ–Ω–æ: {len(specs_list_ua)} —à—Ç.")
        
        ru_url = self._convert_to_ru_url(response.url)
        
        yield scrapy.Request(
            url=ru_url,
            callback=self.parse_product_ru,
            meta={
                **response.meta,
                "name_ua": name_ua,
                "description_ua": description_ua,
                "specifications_list": specs_list_ua,  # –ü–µ—Ä–µ–¥–∞—ë–º —É–∫—Ä–∞–∏–Ω—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
                "original_url": response.url,
            },
            dont_filter=True,
        )
    
    def parse_product_ru(self, response):
        """–ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É —Ç–æ–≤–∞—Ä–∞ (—Ä—É—Å—Å–∫–∞—è –≤–µ—Ä—Å–∏—è) –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ü–µ–ø–æ—á–∫—É"""
        self.logger.info(f"üîó –ü–∞—Ä—Å–∏–º —Ç–æ–≤–∞—Ä (RU): {response.url}")
        
        name_ru = response.css("h1::text").get()
        name_ru = name_ru.strip() if name_ru else ""
        
        description_ru = self._extract_description_with_br(response)
        
        name_ua = response.meta.get("name_ua", "")
        description_ua = response.meta.get("description_ua", "")
        specs_list = response.meta.get("specifications_list", [])
        
        code = ""
        price_raw = response.css("div.card-header__card-price-new::text").get()
        price_raw = price_raw.strip().replace("&nbsp;", "").replace(" ", "") if price_raw else ""
        price = self._clean_price(price_raw) if price_raw else ""
        currency = "UAH"
        
        self.logger.info(f"üìù –û–ø–∏—Å–∞–Ω–∏–µ RU: {len(description_ru)} —Å–∏–º–≤–æ–ª–æ–≤")
        self.logger.info(f"üìù –û–ø–∏—Å–∞–Ω–∏–µ UA: {len(description_ua)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        images = response.css("img.card-header__card-images-image::attr(src)").getall()
        image_url = response.urljoin(images[0]) if images else ""
        
        availability_raw_text = response.css("div.card-header__card-status-badge::text").get()
        availability_status = self._normalize_availability(availability_raw_text)
        quantity = self._extract_quantity(availability_raw_text)
        
        manufacturer = self._extract_manufacturer(name_ru)
        
        search_terms_ru = self._generate_search_terms(name_ru)
        search_terms_ua = self._generate_search_terms(name_ua)
        
        item = {
            "–ö–æ–¥_—Ç–æ–≤–∞—Ä—É": code,
            "–ù–∞–∑–≤–∞_–ø–æ–∑–∏—Ü—ñ—ó": name_ru,
            "–ù–∞–∑–≤–∞_–ø–æ–∑–∏—Ü—ñ—ó_—É–∫—Ä": name_ua,
            "–ü–æ—à—É–∫–æ–≤—ñ_–∑–∞–ø–∏—Ç–∏": search_terms_ru,
            "–ü–æ—à—É–∫–æ–≤—ñ_–∑–∞–ø–∏—Ç–∏_—É–∫—Ä": search_terms_ua,
            "–û–ø–∏—Å": description_ru,
            "–û–ø–∏—Å_—É–∫—Ä": description_ua,
            "–¢–∏–ø_—Ç–æ–≤–∞—Ä—É": "r",
            "–¶—ñ–Ω–∞": price,
            "–í–∞–ª—é—Ç–∞": currency,
            "–û–¥–∏–Ω–∏—Ü—è_–≤–∏–º—ñ—Ä—É": "—à—Ç.",
            "–ü–æ—Å–∏–ª–∞–Ω–Ω—è_–∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è": image_url,
            "–ù–∞—è–≤–Ω—ñ—Å—Ç—å": availability_status,
            "–ö—ñ–ª—å–∫—ñ—Å—Ç—å": quantity,
            "–ù–∞–∑–≤–∞_–≥—Ä—É–ø–∏": response.meta.get("category_ru", ""),
            "–ù–∞–∑–≤–∞_–≥—Ä—É–ø–∏_—É–∫—Ä": response.meta.get("category_ua", ""),
            "–ù–æ–º–µ—Ä_–≥—Ä—É–ø–∏": response.meta.get("group_number", ""),
            "–Ü–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ç–æ—Ä_–ø—ñ–¥—Ä–æ–∑–¥—ñ–ª—É": response.meta.get("subdivision_id", ""),
            "–ü–æ—Å–∏–ª–∞–Ω–Ω—è_–ø—ñ–¥—Ä–æ–∑–¥—ñ–ª—É": response.meta.get("subdivision_link", ""),
            "–í–∏—Ä–æ–±–Ω–∏–∫": manufacturer,
            "–ö—Ä–∞—ó–Ω–∞_–≤–∏—Ä–æ–±–Ω–∏–∫": "",
            "price_type": "retail",
            "–ü—Ä–æ–¥—É–∫—Ç_–Ω–∞_—Å–∞–π—Ç—ñ": response.meta.get("original_url", response.url),
            "specifications_list": specs_list,
        }
        
        self.logger.info(f"‚úÖ YIELD: {item['–ù–∞–∑–≤–∞_–ø–æ–∑–∏—Ü—ñ—ó']} | –¶—ñ–Ω–∞: {item['–¶—ñ–Ω–∞']} | –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫: {len(specs_list)}")
        yield item
        
        # --- –õ–û–ì–ò–ö–ê –¶–ï–ü–û–ß–ö–ò ---
        remaining_products = response.meta.get("remaining_products", [])
        category_index = response.meta.get("category_index")

        if remaining_products:
            # –ï—Å—Ç—å –µ—â–µ –ø—Ä–æ–¥—É–∫—Ç—ã –≤ —ç—Ç–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–π
            next_product_data = remaining_products.pop(0)
            next_product_data["meta"]["remaining_products"] = remaining_products
            next_product_data["meta"]["category_index"] = category_index

            self.logger.info(f"üîó –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ü–µ–ø–æ—á–∫—É –ø—Ä–æ–¥—É–∫—Ç–æ–≤. –û—Å—Ç–∞–ª–æ—Å—å: {len(remaining_products)}")
            yield scrapy.Request(
                url=next_product_data["url"],
                callback=self.parse_product,
                meta=next_product_data["meta"],
                dont_filter=True,
            )
        else:
            # –ü—Ä–æ–¥—É–∫—Ç—ã –≤ —Ç–µ–∫—É—â–µ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∑–∞–∫–æ–Ω—á–∏–ª–∏—Å—å, –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–π
            self.logger.info(f"‚úÖ –í—Å–µ –ø—Ä–æ–¥—É–∫—Ç—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ [{category_index + 1}] –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã.")
            next_request = self._start_next_category(category_index)
            if next_request:
                yield next_request

    def _start_next_category(self, current_category_index):
        """–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Å–ª–µ–¥—É—é—â–µ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        next_category_index = current_category_index + 1
        if next_category_index < len(self.category_urls):
            next_category_url = self.category_urls[next_category_index]
            self.logger.info(f"üöÄ –°–¢–ê–†–¢ –°–õ–ï–î–£–Æ–©–ï–ô –ö–ê–¢–ï–ì–û–†–ò–ò [{next_category_index + 1}/{len(self.category_urls)}]: {next_category_url}")
            return scrapy.Request(
                url=next_category_url,
                callback=self.parse_category,
                meta={
                    "category_url": next_category_url,
                    "category_index": next_category_index,
                    "page_number": 1,
                },
                dont_filter=True,
            )
        else:
            self.logger.info(f"üéâüéâüéâ –í–°–ï –ö–ê–¢–ï–ì–û–†–ò–ò –ò –ü–†–û–î–£–ö–¢–´ –û–ë–†–ê–ë–û–¢–ê–ù–´ üéâüéâüéâ")
            return None
    
    def _clean_price(self, price_str):
        """–û—á–∏—Å—Ç–∫–∞ —Ü–µ–Ω—ã –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        if not price_str:
            return ""
        
        price_str = price_str.replace(" ", "").replace("–≥—Ä–Ω", "").replace("‚Ç¥", "")
        price_str = price_str.replace(",", ".")
        
        try:
            cleaned = "".join(c for c in price_str if c.isdigit() or c == ".")
            return str(float(cleaned)) if cleaned else ""
        except ValueError:
            return ""
    
    def _normalize_availability(self, availability):
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç—É—Å–∞ –Ω–∞–ª–∏—á–∏—è"""
        if not availability:
            return "–£—Ç–æ—á–Ω—è–π—Ç–µ"
        
        availability_lower = availability.lower()
        
        if any(word in availability_lower for word in ["—î –≤ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ", "–≤ –Ω–∞–ª–∏—á–∏–∏", "–µ—Å—Ç—å", "–∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è", "–∑–∞–∫—ñ–Ω—á—É—î—Ç—å—Å—è"]):
            return "–í –Ω–∞–ª–∏—á–∏–∏"
        elif any(word in availability_lower for word in ["–ø—ñ–¥ –∑–∞–º–æ–≤–ª–µ–Ω–Ω—è", "–ø–æ–¥ –∑–∞–∫–∞–∑"]):
            return "–ü–æ–¥ –∑–∞–∫–∞–∑"
        elif any(word in availability_lower for word in ["–Ω–µ–º–∞—î", "–Ω–µ—Ç"]):
            return "–ù–µ—Ç –≤ –Ω–∞–ª–∏—á–∏–∏"
        else:
            return "–£—Ç–æ—á–Ω—è–π—Ç–µ"

    def _extract_quantity(self, text):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –Ω–∞–ª–∏—á–∏—è."""
        if not text:
            return ""  # –ü—É—Å—Ç–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –æ—Å—Ç–∞–µ—Ç—Å—è –ø—É—Å—Ç—ã–º
        
        # –ò—â–µ–º —Ü–∏—Ñ—Ä—ã –≤ —Ç–µ–∫—Å—Ç–µ
        quantity_match = re.search(r'\d+', text)
        if quantity_match:
            return quantity_match.group(0)
        
        # –ï—Å–ª–∏ —Ü–∏—Ñ—Ä –Ω–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É
        return ""
    
    def _generate_search_terms(self, product_name):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: '–ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∞, –°–ª–æ–≤–æ1, –°–ª–æ–≤–æ2, –°–ª–æ–≤–æ3, ...'"""
        if not product_name:
            return ""
        
        words = product_name.replace(",", " ").split()
        
        unique_words = []
        seen = set()
        for word in words:
            word_clean = word.strip().lower()
            if len(word_clean) > 2 and word_clean not in seen:
                unique_words.append(word)
                seen.add(word_clean)
        
        search_terms = f"{product_name}, {', '.join(unique_words)}"
        
        return search_terms
    
    def _extract_specifications(self, response):
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —Ç–æ–≤–∞—Ä–∞ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã (–£–ö–†–ê–ò–ù–°–ö–ò–ï –Ω–∞–∑–≤–∞–Ω–∏—è)
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤: [{'name': '...', 'value': '...', 'unit': ''}, ...]
        """
        specs_list = []
        
        # –ü–æ–ø—ã—Ç–∫–∞ 1: –ê–∫—Ç–∏–≤–Ω–∞—è –≤–∫–ª–∞–¥–∫–∞
        spec_rows = response.css("li.card-tabs__item.active div.card-tabs__characteristic-content table tr")
        
        # –ü–æ–ø—ã—Ç–∫–∞ 2: –õ—é–±–∞—è –≤–∫–ª–∞–¥–∫–∞ —Å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏
        if not spec_rows:
            spec_rows = response.css("div.card-tabs__characteristic-content table tr")
        
        # –ü–æ–ø—ã—Ç–∫–∞ 3: –û–±—â–∏–π —Å–µ–ª–µ–∫—Ç–æ—Ä —Ç–∞–±–ª–∏—Ü—ã
        if not spec_rows:
            spec_rows = response.css("ul.card-tabs__list table tr")
        
        for row in spec_rows[:60]:
            name = row.css("th::text").get()
            value = row.css("td::text").get()
            
            if name and value:
                specs_list.append({
                    "name": name.strip(),
                    "value": value.strip(),
                    "unit": ""
                })
        
        return specs_list
    
    def _convert_to_ru_url(self, url):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —É–∫—Ä–∞–∏–Ω—Å–∫–∏–π URL –≤ —Ä—É—Å—Å–∫–∏–π"""
        if "/ru/" not in url:
            url = url.replace("viatec.ua/", "viatec.ua/ru/")
        return url
    
    def _extract_manufacturer(self, product_name):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–æ–≤–∞—Ä–∞ –∏ –º–∞–ø–ø–∏–Ω–≥–∞ CSV"""
        if not product_name:
            return ""
        
        product_name_lower = product_name.lower()
        
        # –ü–†–ò–û–†–ò–¢–ï–¢ 1: –Ø–≤–Ω—ã–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –±—Ä–µ–Ω–¥–æ–≤ (–¥–ª–∏–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã)
        priority_patterns = {
            "hikvision": "Hikvision",
            "dahua": "Dahua Technology",
            "axis": "Axis",
            "uniview": "UniView",
            "imou": "Imou",
            "ezviz": "Ezviz",
            "unv": "UNV",
            "hiwatch": "HiWatch",
            "ajax": "Ajax",
            "tp-link": "TP-Link",
            "mikrotik": "MikroTik",
            "ubiquiti": "Ubiquiti",
        }
        
        for pattern, name in priority_patterns.items():
            if pattern in product_name_lower:
                return name
        
        # –ü–†–ò–û–†–ò–¢–ï–¢ 2: –ö–æ–¥—ã –ø—Ä–æ–¥—É–∫—Ç–æ–≤ —Å –¥–µ—Ñ–∏—Å–æ–º (–¥–ª—è –º–æ–¥–µ–ª–µ–π)
        code_patterns = {
            "ds-": "Hikvision",
            "dh-": "Dahua Technology",
            "dhi-": "Dahua Technology",
            "vto-": "Dahua Technology",
            "vtm-": "Dahua Technology",
        }
        
        for pattern, name in code_patterns.items():
            if pattern in product_name_lower:
                return name
        
        # –ü–†–ò–û–†–ò–¢–ï–¢ 3: –ú–∞–ø–ø–∏–Ω–≥ –∏–∑ CSV (–∫–æ—Ä–æ—Ç–∫–∏–µ –∫–æ–¥—ã)
        if not hasattr(self, "_manufacturers_cache"):
            self._manufacturers_cache = self._load_manufacturers_from_csv()
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–ª–∏–Ω–µ –∫–ª—é—á–∞ (—Å–Ω–∞—á–∞–ª–∞ –¥–ª–∏–Ω–Ω—ã–µ, –ø–æ—Ç–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ)
        sorted_manufacturers = sorted(
            self._manufacturers_cache.items(),
            key=lambda x: len(x[0]),
            reverse=True
        )
        
        for keyword, manufacturer in sorted_manufacturers:
            keyword_lower = keyword.lower()
            # –î–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –∫–æ–¥–æ–≤ (1-2 —Å–∏–º–≤–æ–ª–∞) —Ç—Ä–µ–±—É–µ–º –≥—Ä–∞–Ω–∏—Ü—ã —Å–ª–æ–≤
            if len(keyword) <= 2:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º word boundary: –ø—Ä–æ–±–µ–ª, –Ω–∞—á–∞–ª–æ/–∫–æ–Ω–µ—Ü —Å—Ç—Ä–æ–∫–∏
                pattern = r'\b' + re.escape(keyword_lower) + r'\b'
                if re.search(pattern, product_name_lower):
                    return manufacturer
            else:
                # –î–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–¥–æ–≤ –ø—Ä–æ—Å—Ç–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ
                if keyword_lower in product_name_lower:
                    return manufacturer
        
        return ""
    
    def _load_manufacturers_from_csv(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–∞–ø–ø–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π –∏–∑ CSV —Ñ–∞–π–ª–∞"""
        mapping = {}
        try:
            csv_path = Path(r"C:\FullStack\Scrapy\data\manufacturers_viatec.csv")
            if csv_path.exists():
                with open(csv_path, encoding="utf-8-sig") as f:
                    reader = csv.DictReader(f, delimiter=";")
                    for row in reader:
                        keyword = row.get("–°–ª–æ–≤–æ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –ø—Ä–æ–¥—É–∫—Ç–∞", "").strip()
                        manufacturer = row.get("–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å (–≤–∏—Ä–æ–±–Ω–∏–∫)", "").strip()
                        if keyword and manufacturer:
                            mapping[keyword] = manufacturer
                self.logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(mapping)} –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π –∏–∑ CSV")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π: {e}")
        
        return mapping
    
    def _extract_description_with_br(self, response):
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ <br> –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Å–ø–∏—Å–∫–æ–≤ <ul>.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç —Å HTML —Ç–µ–≥–∞–º–∏ <br> –¥–ª—è –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ (–¥–ª—è PROM).
        """
        description_container = response.css("div.card-header__card-info-text")
        if not description_container:
            self.logger.warning(f"–ù–µ –Ω–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –æ–ø–∏—Å–∞–Ω–∏—è 'div.card-header__card-info-text' –Ω–∞ {response.url}")
            return ""

        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ <ul>
        ul_list = description_container.css("ul")
        if ul_list:
            self.logger.info(f"–ù–∞–π–¥–µ–Ω <ul> —Å–ø–∏—Å–æ–∫ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏ –Ω–∞ {response.url}")
            list_items = ul_list.css("li")
            
            description_parts = []
            for item in list_items:
                # .get() —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Ç–µ–≥–∏, re.sub —É–±–∏—Ä–∞–µ—Ç <li>
                inner_content = item.get()
                inner_content = re.sub(r'</?li[^>]*>', '', inner_content).strip()
                # –î–æ–±–∞–≤–ª—è–µ–º –º–∞—Ä–∫–µ—Ä, –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
                if not inner_content.startswith('‚óè'):
                    description_parts.append(f"‚óè {inner_content}")
                else:
                    description_parts.append(inner_content)
            
            return "<br>".join(description_parts)

        # 2. –û–±—Ä–∞–±–æ—Ç–∫–∞ <p> —Ç–µ–≥–æ–≤ (—É–ª—É—á—à–µ–Ω–Ω–∞—è —Å—Ç–∞—Ä–∞—è –ª–æ–≥–∏–∫–∞)
        p_tags = description_container.css("p")
        if p_tags:
            self.logger.info(f"–ù–∞–π–¥–µ–Ω—ã <p> —Ç–µ–≥–∏ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏ –Ω–∞ {response.url}")
            result_parts = []
            for p in p_tags:
                if p.css("::attr(class)").get() == "card-header__analog-link":
                    continue
                
                # .get() –≤–µ—Ä–Ω–µ—Ç HTML –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞
                p_html = p.get()
                # –£–±–∏—Ä–∞–µ–º –≤–Ω–µ—à–Ω–∏–µ —Ç–µ–≥–∏ <p>
                inner_html = re.sub(r'^<p[^>]*>|</p>$', '', p_html).strip()
                
                if inner_html:
                    # –ó–∞–º–µ–Ω—è–µ–º <br/> –Ω–∞ <br> –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
                    inner_html = inner_html.replace("<br/>", "<br>").replace("<br />", "<br>")
                    result_parts.append(inner_html)
            
            return "<br>".join(result_parts)
        
        self.logger.warning(f"–í –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ –æ–ø–∏—Å–∞–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∏ <ul>, –Ω–∏ <p> –Ω–∞ {response.url}")
        return ""
    
    def closed(self, reason):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –ø–∞—É–∫–∞ - –∏–∑–¥–∞—ë–º –∑–≤—É–∫–æ–≤–æ–π —Å–∏–≥–Ω–∞–ª"""
        self.logger.info(f"üéâ –ü–∞—É–∫ {self.name} –∑–∞–≤–µ—Ä—à—ë–Ω! –ü—Ä–∏—á–∏–Ω–∞: {reason}")
        
        # –í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º 3 –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–∏–≥–Ω–∞–ª–∞
        try:
            for _ in range(3):
                winsound.Beep(1000, 300)  # –ß–∞—Å—Ç–æ—Ç–∞ 1000 Hz, –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å 300 –º—Å
            self.logger.info("üîî –ó–≤—É–∫–æ–≤–æ–π —Å–∏–≥–Ω–∞–ª –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥—ë–Ω!")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏ –∑–≤—É–∫: {e}")
