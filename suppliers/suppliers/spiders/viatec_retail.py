"""
Spider –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ä–æ–∑–Ω–∏—á–Ω—ã—Ö —Ü–µ–Ω —Å viatec.ua (UAH)
–í—ã–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤: C:\\Users\\stalk\\Documents\\Prom\\prom_import.csv
"""
import scrapy
import csv
from pathlib import Path
from urllib.parse import urljoin


class ViatecRetailSpider(scrapy.Spider):
    name = "viatec_retail"
    allowed_domains = ["viatec.ua"]
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏–∑ CSV
    custom_settings = {
        "CONCURRENT_REQUESTS_PER_DOMAIN": 1,
        "DOWNLOAD_DELAY": 2,
    }
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.category_mapping = self._load_category_mapping()
        self.start_urls = list(self.category_mapping.keys())
    
    def _load_category_mapping(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–∞–ø–ø–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏–∑ CSV"""
        mapping = {}
        csv_path = Path(r"C:\FullStack\Scrapy\data\category_matching_viatec.csv")
        
        try:
            with open(csv_path, encoding="utf-8-sig") as f:
                reader = csv.DictReader(f, delimiter=";")
                for row in reader:
                    url = row["–õ–∏–Ω–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞"].strip().strip('"')
                    mapping[url] = {
                        "category_ru": row["–ö–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–∞ –º–æ–µ–º —Å–∞–π—Ç–µ_RU"],
                        "category_ua": row["–ö–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–∞ –º–æ–µ–º —Å–∞–π—Ç–µ_UA"],
                    }
            self.logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(mapping)} –∫–∞—Ç–µ–≥–æ—Ä–∏–π")
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {e}")
        
        return mapping
    
    def start_requests(self):
        """–°—Ç–∞—Ä—Ç—É–µ–º –ø–∞—Ä—Å–∏–Ω–≥ —Å –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        for url in self.start_urls:
            yield scrapy.Request(
                url=url,
                callback=self.parse_category,
                meta={"category_url": url},
                dont_filter=True,
            )
    
    def parse_category(self, response):
        """–ü–∞—Ä—Å–∏–º —Å–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        category_url = response.meta["category_url"]
        category_info = self.category_mapping.get(category_url, {})
        
        # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Ç–æ–≤–∞—Ä–æ–≤ (–ø—Ä–æ–≤–µ—Ä–µ–Ω–æ test_selectors.py)
        product_links = response.css("a[href*='/product/']::attr(href)").getall()
        
        if not product_links:
            self.logger.warning(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω—ã —Ç–æ–≤–∞—Ä—ã –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category_url}")
        
        for link in product_links:
            product_url = response.urljoin(link)
            yield scrapy.Request(
                url=product_url,
                callback=self.parse_product,
                meta={
                    "category_url": category_url,
                    "category_ru": category_info.get("category_ru", ""),
                    "category_ua": category_info.get("category_ua", ""),
                },
            )
        
        # –ü–∞–≥–∏–Ω–∞—Ü–∏—è - —Å–µ–ª–µ–∫—Ç–æ—Ä: <a href="https://viatec.ua/catalog/cameras/proizvoditel:hikvision;page:2" class="paggination__page">2</a>
        next_page_link = response.css("a.paggination__next::attr(href)").get()
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ —Å–ª–µ–¥—É—é—â–∞—è –∫–Ω–æ–ø–∫–∞, –∏—â–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤ –Ω—É–º–µ—Ä–∞—Ü–∏–∏
        if not next_page_link:
            all_pages = response.css("a.paggination__page::attr(href)").getall()
            active_page = response.css("a.paggination__page--active::text").get()
            if all_pages and active_page:
                try:
                    current_idx = response.css("a.paggination__page").index(
                        response.css("a.paggination__page--active")[0]
                    ) if hasattr(response.css("a.paggination__page"), "index") else -1
                    if current_idx >= 0 and current_idx + 1 < len(all_pages):
                        next_page_link = all_pages[current_idx + 1]
                except:
                    pass
        
        next_page = next_page_link
        
        if next_page:
            self.logger.info(f"üìÑ –ù–∞–π–¥–µ–Ω–∞ —Å–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {next_page}")
            yield response.follow(
                next_page,
                callback=self.parse_category,
                meta={"category_url": category_url},
            )
        else:
            self.logger.info(f"‚úÖ –ü–∞–≥–∏–Ω–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category_url}")
    
    def parse_product(self, response):
        """–ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É —Ç–æ–≤–∞—Ä–∞"""
        self.logger.info(f"üîó –ü–∞—Ä—Å–∏–º —Ç–æ–≤–∞—Ä: {response.url}")
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Ç–æ–≤–∞—Ä–∞
        name_ru = response.css("h1::text").get()
        name_ru = name_ru.strip() if name_ru else ""
        name_ua = name_ru
        
        # –ö–æ–¥ —Ç–æ–≤–∞—Ä–∞ - –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏, –Ω–∞—á–∏–Ω–∞—è —Å 200000
        code = self._generate_product_code(response)
        
        # –¶–µ–Ω–∞ - —Å–µ–ª–µ–∫—Ç–æ—Ä –∏–∑ HTML: <div class="card-header__card-price-new">2&nbsp;537 <span>–≥—Ä–Ω</span></div>
        price_raw = response.css("div.card-header__card-price-new::text").get()
        if price_raw:
            price_raw = price_raw.strip().replace("&nbsp;", "").replace(" ", "")
        else:
            price_raw = ""
        
        price = self._clean_price(price_raw) if price_raw else ""
        
        # –í–∞–ª—é—Ç–∞
        currency = "UAH"
        
        # –û–ø–∏—Å–∞–Ω–∏–µ - —Å–µ–ª–µ–∫—Ç–æ—Ä: <p>‚óè –†–æ–∑–¥—ñ–ª—å–Ω–∞ –∑–¥–∞—Ç–Ω—ñ—Å—Ç—å...</p>
        description_raw = response.css("div.card-header__card-description p::text, div.card-header__card-description p *::text").getall()
        if not description_raw:
            description_raw = response.css("div.card-header__card-description::text").getall()
        description_ru = " ".join([d.strip() for d in description_raw if d.strip()])
        description_ua = description_ru
        
        self.logger.info(f"üìù –û–ø–∏—Å–∞–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–æ: {len(description_ru)} —Å–∏–º–≤–æ–ª–æ–≤")
        self.logger.debug(f"üìÑ –¢–µ–∫—Å—Ç –æ–ø–∏—Å–∞–Ω–∏—è: {description_ru[:100]}...")
        
        # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è - —Å–µ–ª–µ–∫—Ç–æ—Ä: <img src="/upload/images/prod/2024-06/DS-2CD1321G0-I.webp" class="card-header__card-images-image">
        images = response.css("img.card-header__card-images-image::attr(src)").getall()
        image_url = response.urljoin(images[0]) if images else ""
        
        # –ù–∞–ª–∏—á–∏–µ - —Å–µ–ª–µ–∫—Ç–æ—Ä: <div class="card-header__card-status-badge">–í –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ</div>
        availability = response.css("div.card-header__card-status-badge::text").get()
        availability = self._normalize_availability(availability)
        
        # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å - –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–∑ –º–∞–ø–ø–∏–Ω–≥–∞ –ø–æ URL
        manufacturer = self._extract_manufacturer_from_url(response.url)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        search_terms_ru = self._generate_search_terms(name_ru)
        search_terms_ua = self._generate_search_terms(name_ua)
        
        # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        specs = self._extract_specifications(response)
        self.logger.info(f"üìê –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏: {len([k for k in specs.keys() if '–ù–∞–∑–≤–∞_–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏' in k])} —à—Ç.")
        
        item = {
            "–ö–æ–¥_—Ç–æ–≤–∞—Ä—É": code,
            "–ù–∞–∑–≤–∞_–ø–æ–∑–∏—Ü—ñ—ó": name_ru,
            "–ù–∞–∑–≤–∞_–ø–æ–∑–∏—Ü—ñ—ó_—É–∫—Ä": name_ua,
            "–ü–æ—à—É–∫–æ–≤—ñ_–∑–∞–ø–∏—Ç–∏": search_terms_ru,
            "–ü–æ—à—É–∫–æ–≤—ñ_–∑–∞–ø–∏—Ç–∏_—É–∫—Ä": search_terms_ua,
            "–û–ø–∏—Å": description_ru,
            "–û–ø–∏—Å_—É–∫—Ä": description_ua,
            "–¶—ñ–Ω–∞": price,
            "–í–∞–ª—é—Ç–∞": currency,
            "–û–¥–∏–Ω–∏—Ü—è_–≤–∏–º—ñ—Ä—É": "—à—Ç.",
            "–ü–æ—Å–∏–ª–∞–Ω–Ω—è_–∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è": image_url,
            "–ù–∞—è–≤–Ω—ñ—Å—Ç—å": availability,
            "–ù–∞–∑–≤–∞_–≥—Ä—É–ø–∏": response.meta.get("category_ru", ""),
            "–ù–∞–∑–≤–∞_–≥—Ä—É–ø–∏_—É–∫—Ä": response.meta.get("category_ua", ""),
            "–í–∏—Ä–æ–±–Ω–∏–∫": manufacturer,
            "–ö—Ä–∞—ó–Ω–∞_–≤–∏—Ä–æ–±–Ω–∏–∫": "",
            "price_type": "retail",  # –ú–∞—Ä–∫–µ—Ä –¥–ª—è pipeline
            "–ü—Ä–æ–¥—É–∫—Ç_–Ω–∞_—Å–∞–π—Ç—ñ": response.url,
            **specs,  # –î–æ–±–∞–≤–ª—è–µ–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        }
        
        self.logger.info(f"‚úÖ YIELD: {item['–ù–∞–∑–≤–∞_–ø–æ–∑–∏—Ü—ñ—ó']} | –¶—ñ–Ω–∞: {item['–¶—ñ–Ω–∞']} | –ù–∞—è–≤–Ω—ñ—Å—Ç—å: {item['–ù–∞—è–≤–Ω—ñ—Å—Ç—å']}")
        yield item
    
    def _clean_price(self, price_str):
        """–û—á–∏—Å—Ç–∫–∞ —Ü–µ–Ω—ã –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        if not price_str:
            return ""
        
        # –£–¥–∞–ª—è–µ–º –≤—Å—ë –∫—Ä–æ–º–µ —Ü–∏—Ñ—Ä, —Ç–æ—á–∫–∏ –∏ –∑–∞–ø—è—Ç–æ–π
        price_str = price_str.replace(" ", "").replace("–≥—Ä–Ω", "").replace("‚Ç¥", "")
        price_str = price_str.replace(",", ".")
        
        try:
            cleaned = "".join(c for c in price_str if c.isdigit() or c == ".")
            return str(float(cleaned)) if cleaned else ""
        except ValueError:
            return ""
    
    def _normalize_availability(self, availability):
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç—É—Å–∞ –Ω–∞–ª–∏—á–∏—è"""
        if not availability:
            return "–£—Ç–æ—á–Ω—è–π—Ç–µ"
        
        availability_lower = availability.lower()
        
        if any(word in availability_lower for word in ["—î –≤ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ", "–≤ –Ω–∞–ª–∏—á–∏–∏", "–µ—Å—Ç—å"]):
            return "–í –Ω–∞–ª–∏—á–∏–∏"
        elif any(word in availability_lower for word in ["–ø—ñ–¥ –∑–∞–º–æ–≤–ª–µ–Ω–Ω—è", "–ø–æ–¥ –∑–∞–∫–∞–∑"]):
            return "–ü–æ–¥ –∑–∞–∫–∞–∑"
        elif any(word in availability_lower for word in ["–Ω–µ–º–∞—î", "–Ω–µ—Ç"]):
            return "–ù–µ—Ç –≤ –Ω–∞–ª–∏—á–∏–∏"
        else:
            return "–£—Ç–æ—á–Ω—è–π—Ç–µ"
    
    def _generate_search_terms(self, product_name):
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤:
        '–ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∞, –°–ª–æ–≤–æ1, –°–ª–æ–≤–æ2, –°–ª–æ–≤–æ3, ...'
        """
        if not product_name:
            return ""
        
        # –ü–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ + —Å–ª–æ–≤–∞ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é
        words = product_name.replace(",", " ").split()
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
        unique_words = []
        seen = set()
        for word in words:
            word_clean = word.strip().lower()
            if len(word_clean) > 2 and word_clean not in seen:
                unique_words.append(word)
                seen.add(word_clean)
        
        # –§–æ—Ä–º–∞—Ç: "–ü–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ, –°–ª–æ–≤–æ1, –°–ª–æ–≤–æ2, ..."
        search_terms = f"{product_name}, {', '.join(unique_words)}"
        
        return search_terms
    
    def _extract_specifications(self, response):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —Ç–æ–≤–∞—Ä–∞ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã"""
        specs = {}
        
        # –°–µ–ª–µ–∫—Ç–æ—Ä –¥–ª—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞:
        # <table><tbody><tr><th>–ú–∞—Ç—Ä–∏—Ü—è</th><td>1/2.9" Progressive Scan CMOS</td></tr>...</table>
        spec_rows = response.css("div.card-tabs__characteristic-content table tbody tr")
        
        for i, row in enumerate(spec_rows[:30], 1):  # –ú–∞–∫—Å–∏–º—É–º 30 —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
            # –ù–∞–∑–≤–∞–Ω–∏—è –≤ <th>, –∑–Ω–∞—á–µ–Ω–∏—è –≤ <td>
            name = row.css("th::text").get()
            value = row.css("td::text").get()
            
            if name and value:
                specs[f"–ù–∞–∑–≤–∞_–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏_{i}"] = name.strip()
                specs[f"–ó–Ω–∞—á–µ–Ω–Ω—è_–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏_{i}"] = value.strip()
                specs[f"–û–¥–∏–Ω–∏—Ü—è_–≤–∏–º—ñ—Ä—É_–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏_{i}"] = ""
        
        return specs
    
    def _generate_product_code(self, response):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–¥–∞ —Ç–æ–≤–∞—Ä–∞, –Ω–∞—á–∏–Ω–∞—è —Å 200000"""
        # –ü–æ–ª—É—á–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –∏–∑ spider –∞—Ç—Ä–∏–±—É—Ç–∞
        if not hasattr(self, "_product_counter"):
            self._product_counter = 200000
        
        code = str(self._product_counter)
        self._product_counter += 1
        return code
    
    def _extract_manufacturer_from_url(self, url):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è –∏–∑ URL –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏ –º–∞–ø–ø–∏–Ω–≥–∞ CSV"""
        url_lower = url.lower()
        
        # –ö—ç—à –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ CSV
        if not hasattr(self, "_manufacturers_cache"):
            self._manufacturers_cache = self._load_manufacturers_from_csv()
        
        # –ò—â–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è –≤ CSV –ø–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é —Å URL
        for keyword, manufacturer in self._manufacturers_cache.items():
            if keyword.lower() in url_lower:
                return manufacturer
        
        # –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π
        url_patterns = {
            "hikvision": "Hikvision",
            "dahua": "Dahua Technology",
            "axis": "Axis",
            "uniview": "UniView",
            "imou": "Imou",
            "ezviz": "Ezviz",
            "unv": "UNV",
        }
        
        for pattern, name in url_patterns.items():
            if pattern in url_lower:
                return name
        
        return ""
    
    def _load_manufacturers_from_csv(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–∞–ø–ø–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π –∏–∑ CSV —Ñ–∞–π–ª–∞"""
        mapping = {}
        try:
            csv_path = Path(r"C:\FullStack\Scrapy\data\manufacturers_viatec.csv")
            if csv_path.exists():
                with open(csv_path, encoding="utf-8-sig") as f:
                    reader = csv.DictReader(f, delimiter=";")
                    for row in reader:
                        # CSV —Å—Ç—Ä—É–∫—Ç—É—Ä–∞: '–°–ª–æ–≤–æ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –ø—Ä–æ–¥—É–∫—Ç–∞;–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å (–≤–∏—Ä–æ–±–Ω–∏–∫)'
                        keyword = row.get("–°–ª–æ–≤–æ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –ø—Ä–æ–¥—É–∫—Ç–∞", "").strip()
                        manufacturer = row.get("–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å (–≤–∏—Ä–æ–±–Ω–∏–∫)", "").strip()
                        if keyword and manufacturer:
                            mapping[keyword] = manufacturer
                self.logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(mapping)} –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π –∏–∑ CSV")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π: {e}")
        
        return mapping
