"""
Spider –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É —Ä–æ–∑–¥—Ä—ñ–±–Ω–∏—Ö —Ü—ñ–Ω –∑ e-server.com.ua (UAH)
–í–∏–≥—Ä—É–∂–∞—î –¥–∞–Ω—ñ –≤: output/eserver_retail.csv

–ü–û–°–õ–Ü–î–û–í–ù–ê –û–ë–†–û–ë–ö–ê: –∫–∞—Ç–µ–≥–æ—Ä—ñ—è ‚Üí –≤—Å—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –ø–∞–≥—ñ–Ω–∞—Ü—ñ—ó ‚Üí –Ω–∞—Å—Ç—É–ø–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è
–•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö–ò: –ø–∞—Ä—Å—è—Ç—å—Å—è –£–ö–†–ê–á–ù–°–¨–ö–û–Æ (UA) —Ç–∞ –†–û–°–Ü–ô–°–¨–ö–û–Æ (RU) –∑ –æ–∫—Ä–µ–º–∏—Ö URL
–ü–ê–ì–Ü–ù–ê–¶–Ü–Ø: –ü—ñ–¥—Ç—Ä–∏–º–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ ?only-inStock —Ç–∞ &page=N
"""
import scrapy
import csv
import re
from pathlib import Path
from suppliers.spiders.base import EserverBaseSpider, BaseRetailSpider


class EserverRetailSpider(EserverBaseSpider, BaseRetailSpider):
    name = "eserver_retail"
    supplier_id = "eserver"
    output_filename = "eserver_retail.csv"
    
    custom_settings = {
        **EserverBaseSpider.custom_settings,
        "ITEM_PIPELINES": {
            "suppliers.pipelines.SuppliersPipeline": 300,
        },
    }
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.category_mapping = self._load_category_mapping()
        self.category_urls = list(self.category_mapping.keys())
        self.current_category_index = 0
        self.keywords_mapping = self._load_keywords_mapping_eserver()
    
    def _load_category_mapping(self):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –º–∞–ø–ø—ñ–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä—ñ–π –∑ CSV"""
        mapping = {}
        csv_path = Path(r"C:\FullStack\Scrapy\data\eserver\eserver_category_retail.csv")
        
        try:
            with open(csv_path, encoding="utf-8-sig") as f:
                reader = csv.DictReader(f, delimiter=";")
                for row in reader:
                    url = row["–õ–∏–Ω–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞"].strip().strip('"')
                    
                    if not url or url == "" or not url.startswith("http"):
                        continue
                    
                    mapping[url] = {
                        "category_ru": row["–ö–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–∞ –º–æ–µ–º —Å–∞–π—Ç–µ_RU"],
                        "category_ua": row["–ö–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–∞ –º–æ–µ–º —Å–∞–π—Ç–µ_UA"],
                        "group_number": row.get("–ù–æ–º–µ—Ä_–≥—Ä—É–ø–∏", ""),
                        "subdivision_id": row.get("–Ü–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ç–æ—Ä_–ø—ñ–¥—Ä–æ–∑–¥—ñ–ª—É", ""),
                        "subdivision_link": row.get("–ü–æ—Å–∏–ª–∞–Ω–Ω—è_–ø—ñ–¥—Ä–æ–∑–¥—ñ–ª—É", ""),
                    }
            self.logger.info(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(mapping)} –∫–∞—Ç–µ–≥–æ—Ä—ñ–π")
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–π: {e}")
        
        return mapping
    
    def start_requests(self):
        """–°—Ç–∞—Ä—Ç—É—î–º–æ –∑ –ø–µ—Ä—à–æ—ó –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó"""
        if self.category_urls:
            first_category_url = self.category_urls[0]
            self.logger.info(f"üöÄ –°–¢–ê–†–¢ –ü–ê–†–°–ò–ù–ì–£. –ü–µ—Ä—à–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è [1/{len(self.category_urls)}]: {first_category_url}")
            
            yield scrapy.Request(
                url=first_category_url,
                callback=self.parse_category,
                meta={
                    "category_url": first_category_url,
                    "category_index": 0,
                    "page_number": 1,
                },
                dont_filter=True,
            )
    
    def parse_category(self, response):
        """–ü–∞—Ä—Å–∏–º–æ —Å–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä—ñ–≤ —É –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó —Ç–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –ø–∞–≥—ñ–Ω–∞—Ü—ñ—ó"""
        category_url = response.meta["category_url"]
        category_index = response.meta["category_index"]
        page_number = response.meta.get("page_number", 1)
        category_info = self.category_mapping.get(category_url, {})
        
        self.logger.info(f"üìÇ –û–±—Ä–æ–±–ª—è—é –∫–∞—Ç–µ–≥–æ—Ä—ñ—é [{category_index + 1}/{len(self.category_urls)}] —Å—Ç–æ—Ä—ñ–Ω–∫–∞ {page_number}: {response.url}")
        
        # –ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ —Ç–æ–≤–∞—Ä–∏ - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —É–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∏–π —Å–µ–ª–µ–∫—Ç–æ—Ä –∫–∞—Ä—Ç–æ—á–æ–∫
        # –°–∞–π—Ç –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î —Ä—ñ–∑–Ω—ñ URL —Å—Ç—Ä—É–∫—Ç—É—Ä–∏: –∑ -detail —Ç–∞ –±–µ–∑
        product_links = response.css("div[class*='card'] a[href*='/uk/']::attr(href)").getall()
        
        if not product_links:
            self.logger.warning(f"‚ö†Ô∏è –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä—ñ–≤ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ: {response.url}")
        else:
            self.logger.info(f"üì¶ –ó–Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä—ñ–≤ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ: {len(product_links)}")
            for link in product_links:
                product_url = response.urljoin(link)
                
                if product_url not in self.processed_products:
                    self.products_from_pagination.append({
                        "url": product_url,
                        "meta": {
                            "category_url": category_url,
                            "category_ru": category_info.get("category_ru", ""),
                            "category_ua": category_info.get("category_ua", ""),
                            "group_number": category_info.get("group_number", ""),
                            "subdivision_id": category_info.get("subdivision_id", ""),
                            "subdivision_link": category_info.get("subdivision_link", ""),
                        },
                    })
                    self.processed_products.add(product_url)
        
        # –ü–ê–ì–Ü–ù–ê–¶–Ü–Ø
        next_page_link = response.css("li.next a::attr(href)").get()
        
        if not next_page_link and len(product_links) > 0:
            next_page_link = self._build_next_page_url(category_url, page_number, len(product_links))
        
        if next_page_link:
            self.logger.info(f"üìÑ –ü–µ—Ä–µ—Ö—ñ–¥ –Ω–∞ –Ω–∞—Å—Ç—É–ø–Ω—É —Å—Ç–æ—Ä—ñ–Ω–∫—É –ø–∞–≥—ñ–Ω–∞—Ü—ñ—ó ({page_number + 1}): {next_page_link}")
            
            yield response.follow(
                url=next_page_link,
                callback=self.parse_category,
                meta={
                    "category_url": category_url,
                    "category_index": category_index,
                    "page_number": page_number + 1,
                },
                dont_filter=True,
            )
        else:
            self.logger.info(f"‚úÖ –ü–ê–ì–Ü–ù–ê–¶–Ü–Ø –ó–ê–í–ï–†–®–ï–ù–ê [{category_index + 1}/{len(self.category_urls)}]: –Ω–∞–∫–æ–ø–∏—á–µ–Ω–æ {len(self.products_from_pagination)} —Ç–æ–≤–∞—Ä—ñ–≤")
            
            if self.products_from_pagination:
                product_data = self.products_from_pagination.pop(0)
                product_data["meta"]["remaining_products"] = self.products_from_pagination
                product_data["meta"]["category_index"] = category_index
                
                self.logger.info(f"üîó –ó–ê–ü–£–°–ö –ª–∞–Ω—Ü—é–≥–∞ –ø—Ä–æ–¥—É–∫—Ç—ñ–≤. –ü–µ—Ä—à–∏–π: {product_data['url']}. –ó–∞–ª–∏—à–∏–ª–æ—Å—å: {len(self.products_from_pagination)}")
                
                yield scrapy.Request(
                    url=product_data["url"],
                    callback=self.parse_product,
                    errback=self.parse_product_error,
                    meta=product_data["meta"],
                    dont_filter=True,
                )
            else:
                self.logger.warning(f"‚ö†Ô∏è –£ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó {category_url} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä—ñ–≤. –ü–µ—Ä–µ—Ö–æ–¥–∂—É –¥–æ –Ω–∞—Å—Ç—É–ø–Ω–æ—ó.")
                yield self._start_next_category(category_index)
            
            self.products_from_pagination = []
    
    def _build_next_page_url(self, category_url, current_page, products_count):
        """–ë—É–¥—É—î URL –Ω–∞—Å—Ç—É–ø–Ω–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏"""
        if products_count == 0:
            return None
        
        next_page_number = current_page + 1
        
        if '/page/' in category_url:
            return re.sub(r'/page/\d+', f'/page/{next_page_number}', category_url)
        else:
            clean_url = category_url.rstrip('/')
            return f"{clean_url}/page/{next_page_number}"
    
    def parse_product(self, response):
        """–ü–∞—Ä—Å–∏–º–æ —Å—Ç–æ—Ä—ñ–Ω–∫—É —Ç–æ–≤–∞—Ä—É - —à—É–∫–∞—î–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ –æ–±–∏–¥–≤—ñ –º–æ–≤–∏ —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–∏–∫–∞—á"""
        try:
            self.logger.info(f"üîó –ü–∞—Ä—Å–∏–º–æ —Ç–æ–≤–∞—Ä (–ø–æ—à—É–∫ –º–æ–≤): {response.url}")
            
            # –®—É–∫–∞—î–º–æ –ø–µ—Ä–µ–º–∏–∫–∞—á –º–æ–≤–∏
            # –°–µ–ª–µ–∫—Ç–æ—Ä: <a href="/uk/..."><div>–£–∫—Ä</div></a>
            # –°–µ–ª–µ–∫—Ç–æ—Ä: <a href="/servernye-shkafy/..."><div>–†—É—Å</div></a>
            ua_link = response.css("div.langs_langs__QyR6J a[href*='/uk/']::attr(href)").get()
            ru_link = response.css("div.langs_langs__QyR6J a:not([href*='/uk/'])::attr(href)").get()
            
            if not ua_link or not ru_link:
                self.logger.error(f"‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –ø–æ—Å–∏–ª–∞–Ω—å –Ω–∞ –º–æ–≤–∏: UA={ua_link}, RU={ru_link}")
                yield from self._skip_product(response.meta)
                return
            
            # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ URL
            ua_url = response.urljoin(ua_link)
            ru_url = response.urljoin(ru_link)
            
            self.logger.info(f"üåê –ó–Ω–∞–π–¥–µ–Ω–æ –º–æ–≤–∏: UA={ua_url}, RU={ru_url}")
            
            # –ü–µ—Ä–µ—Ö–æ–¥–∏–º–æ –Ω–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É –≤–µ—Ä—Å—ñ—é
            yield scrapy.Request(
                url=ua_url,
                callback=self.parse_product_ua,
                errback=self.parse_product_error,
                meta={
                    **response.meta,
                    "ru_url": ru_url,
                    "original_url": response.url,
                },
                dont_filter=True,
            )
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É –ø–µ—Ä–µ–º–∏–∫–∞—á–∞ –º–æ–≤: {response.url} | {e}")
            yield from self._skip_product(response.meta)
            return
    
    def parse_product_ua(self, response):
        """–ü–∞—Ä—Å–∏–º–æ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É –≤–µ—Ä—Å—ñ—é —Ç–æ–≤–∞—Ä—É"""
        try:
            self.logger.info(f"üîó –ü–∞—Ä—Å–∏–º–æ —Ç–æ–≤–∞—Ä (UA): {response.url}")
            
            # –°–µ–ª–µ–∫—Ç–æ—Ä –¥–ª—è h1 –∑ –∫–ª–∞—Å–æ–º es-h1
            name_ua = response.css("h1.es-h1::text").get()
            if not name_ua:
                name_ua = response.css("h1::text").get()
            name_ua = name_ua.strip() if name_ua else ""
            
            description_ua = self._extract_description_from_html(response)
            specs_list_ua = self._extract_specifications_eserver(response)
            
            self.logger.info(f"üìä –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ (UA) –∑–Ω–∞–π–¥–µ–Ω–æ: {len(specs_list_ua)} —à—Ç.")
            
            ru_url = response.meta.get("ru_url")
            
            # –ü–µ—Ä–µ—Ö–æ–¥–∏–º–æ –Ω–∞ —Ä–æ—Å—ñ–π—Å—å–∫—É –≤–µ—Ä—Å—ñ—é
            yield scrapy.Request(
                url=ru_url,
                callback=self.parse_product_ru,
                errback=self.parse_product_error,
                meta={
                    **response.meta,
                    "name_ua": name_ua,
                    "description_ua": description_ua,
                    "specifications_list": specs_list_ua,
                },
                dont_filter=True,
            )
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É –ø—Ä–æ–¥—É–∫—Ç—É (UA): {response.url} | {e}")
            yield from self._skip_product(response.meta)
            return
    
    def parse_product_ru(self, response):
        """–ü–∞—Ä—Å–∏–º–æ —Ä–æ—Å—ñ–π—Å—å–∫—É –≤–µ—Ä—Å—ñ—é —Ç–æ–≤–∞—Ä—É —Ç–∞ –ø—Ä–æ–¥–æ–≤–∂—É—î–º–æ –ª–∞–Ω—Ü—é–≥"""
        try:
            self.logger.info(f"üîó –ü–∞—Ä—Å–∏–º–æ —Ç–æ–≤–∞—Ä (RU): {response.url}")
            
            # –°–µ–ª–µ–∫—Ç–æ—Ä –¥–ª—è h1 –∑ –∫–ª–∞—Å–æ–º es-h1
            name_ru = response.css("h1.es-h1::text").get()
            if not name_ru:
                name_ru = response.css("h1::text").get()
            name_ru = name_ru.strip() if name_ru else ""
            
            description_ru = self._extract_description_from_html(response)
            
            name_ua = response.meta.get("name_ua", "")
            description_ua = response.meta.get("description_ua", "")
            specs_list = response.meta.get("specifications_list", [])
            
            # –¶—ñ–Ω–∞
            price_raw = response.css("div.flex.items-end.font-bold.text-23px::text").get()
            if not price_raw:
                price_raw = response.css("div[class*='price']::text").get()
            price = self._clean_price(price_raw) if price_raw else ""
            
            # –ù–∞—è–≤–Ω—ñ—Å—Ç—å - –†–û–ó–®–ò–†–ï–ù–ï –í–ò–¢–Ø–ì–£–í–ê–ù–ù–Ø –ó –õ–û–ì–£–í–ê–ù–ù–Ø–ú
            availability_raw = ""
            
            # –°–ø—Ä–æ–±—É—î–º–æ —Ä—ñ–∑–Ω—ñ —Å–µ–ª–µ–∫—Ç–æ—Ä–∏
            availability_element = response.css("div.product_ag-sts__x60QA")
            if availability_element:
                availability_text = availability_element.css("::text").getall()
                availability_raw = " ".join([t.strip() for t in availability_text if t.strip()])
                self.logger.info(f"üì¶ –ù–∞—è–≤–Ω—ñ—Å—Ç—å (—Å–µ–ª–µ–∫—Ç–æ—Ä 1): '{availability_raw}'")
            
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∏–π —Å–µ–ª–µ–∫—Ç–æ—Ä 1: –∑–∞–≥–∞–ª—å–Ω–∏–π –ø–æ—à—É–∫ —Ç–µ–∫—Å—Ç—É –∑ "–Ω–∞—è–≤–Ω–æ—Å—Ç—ñ" –∞–±–æ "–Ω–∞–ª–∏—á–∏–∏"
            if not availability_raw:
                all_text = response.css("*::text").getall()
                for text in all_text:
                    text_lower = text.lower().strip()
                    if "–Ω–∞—è–≤–Ω–æ—Å—Ç" in text_lower or "–Ω–∞–ª–∏—á" in text_lower:
                        availability_raw = text.strip()
                        self.logger.info(f"üì¶ –ù–∞—è–≤–Ω—ñ—Å—Ç—å (—Å–µ–ª–µ–∫—Ç–æ—Ä 2 - –ø–æ—à—É–∫): '{availability_raw}'")
                        break
            
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∏–π —Å–µ–ª–µ–∫—Ç–æ—Ä 2: —à—É–∫–∞—î–º–æ –≤ div –∑ –∫–ª–∞—Å–∞–º–∏ —â–æ –º—ñ—Å—Ç—è—Ç—å "status", "stock", "available"
            if not availability_raw:
                status_divs = response.css("div[class*='status'], div[class*='stock'], div[class*='available']")
                for div in status_divs:
                    text = " ".join(div.css("::text").getall()).strip()
                    if text:
                        availability_raw = text
                        self.logger.info(f"üì¶ –ù–∞—è–≤–Ω—ñ—Å—Ç—å (—Å–µ–ª–µ–∫—Ç–æ—Ä 3 - div): '{availability_raw}'")
                        break
            
            # –Ø–∫—â–æ –Ω—ñ—á–æ–≥–æ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ - –ª–æ–≥—É—î–º–æ –ø–æ–ø–µ—Ä–µ–¥–∂–µ–Ω–Ω—è –∑ HTML
            if not availability_raw:
                self.logger.warning(f"‚ö†Ô∏è –ù–ï –ó–ù–ê–ô–î–ï–ù–û –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ –¥–ª—è: {response.url}")
                # –õ–æ–≥—É—î–º–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç HTML –¥–ª—è –¥–µ–±–∞–≥—É
                product_section = response.css("div[class*='product']").get()
                if product_section:
                    self.logger.warning(f"HTML —Ñ—Ä–∞–≥–º–µ–Ω—Ç: {product_section[:500]}...")
                # –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º –≤–≤–∞–∂–∞—î–º–æ –í –ù–ê–Ø–í–ù–û–°–¢–Ü (–±–æ –≤ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó —Ñ—ñ–ª—å—Ç—Ä only-inStock)
                availability_raw = "–í –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ"
            
            # –ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è
            image_url = self._extract_image_from_srcset(response)
            
            # –í–∏—Ä–æ–±–Ω–∏–∫
            manufacturer = self._extract_manufacturer(name_ru)
            
            # –ü–æ—à—É–∫–æ–≤—ñ –∑–∞–ø–∏—Ç–∏ –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤
            subdivision_id = response.meta.get("subdivision_id", "")
            search_terms_ru = self._generate_search_terms(name_ru, subdivision_id, lang="ru")
            search_terms_ua = self._generate_search_terms(name_ua, subdivision_id, lang="ua")
            
            # –ö—ñ–ª—å–∫—ñ—Å—Ç—å
            quantity = self._extract_quantity(availability_raw)
            
            self.logger.info(f"üìù –û–ø–∏—Å RU: {len(description_ru)} —Å–∏–º–≤–æ–ª—ñ–≤")
            self.logger.info(f"üìù –û–ø–∏—Å UA: {len(description_ua)} —Å–∏–º–≤–æ–ª—ñ–≤")
            
            item = {
                "–ö–æ–¥_—Ç–æ–≤–∞—Ä—É": "",
                "–ù–∞–∑–≤–∞_–ø–æ–∑–∏—Ü—ñ—ó": name_ru,
                "–ù–∞–∑–≤–∞_–ø–æ–∑–∏—Ü—ñ—ó_—É–∫—Ä": name_ua,
                "–ü–æ—à—É–∫–æ–≤—ñ_–∑–∞–ø–∏—Ç–∏": search_terms_ru,
                "–ü–æ—à—É–∫–æ–≤—ñ_–∑–∞–ø–∏—Ç–∏_—É–∫—Ä": search_terms_ua,
                "–û–ø–∏—Å": description_ru,
                "–û–ø–∏—Å_—É–∫—Ä": description_ua,
                "–¢–∏–ø_—Ç–æ–≤–∞—Ä—É": "r",
                "–¶—ñ–Ω–∞": price,
                "–í–∞–ª—é—Ç–∞": self.currency,
                "–û–¥–∏–Ω–∏—Ü—è_–≤–∏–º—ñ—Ä—É": "—à—Ç.",
                "–ü–æ—Å–∏–ª–∞–Ω–Ω—è_–∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è": image_url,
                "–ù–∞—è–≤–Ω—ñ—Å—Ç—å": availability_raw,
                "–ö—ñ–ª—å–∫—ñ—Å—Ç—å": quantity,
                "–ù–∞–∑–≤–∞_–≥—Ä—É–ø–∏": response.meta.get("category_ru", ""),
                "–ù–∞–∑–≤–∞_–≥—Ä—É–ø–∏_—É–∫—Ä": response.meta.get("category_ua", ""),
                "–ù–æ–º–µ—Ä_–≥—Ä—É–ø–∏": response.meta.get("group_number", ""),
                "–Ü–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ç–æ—Ä_–ø—ñ–¥—Ä–æ–∑–¥—ñ–ª—É": response.meta.get("subdivision_id", ""),
                "–ü–æ—Å–∏–ª–∞–Ω–Ω—è_–ø—ñ–¥—Ä–æ–∑–¥—ñ–ª—É": response.meta.get("subdivision_link", ""),
                "–í–∏—Ä–æ–±–Ω–∏–∫": manufacturer,
                "–ö—Ä–∞—ó–Ω–∞_–≤–∏—Ä–æ–±–Ω–∏–∫": "",
                "price_type": self.price_type,
                "supplier_id": self.supplier_id,
                "output_file": self.output_filename,
                "–ü—Ä–æ–¥—É–∫—Ç_–Ω–∞_—Å–∞–π—Ç—ñ": response.meta.get("original_url", response.url),
                "specifications_list": specs_list,
            }
            
            self.logger.info(f"‚úÖ YIELD: {item['–ù–∞–∑–≤–∞_–ø–æ–∑–∏—Ü—ñ—ó']} | –¶—ñ–Ω–∞: {item['–¶—ñ–Ω–∞']} | –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫: {len(specs_list)}")
            yield item
            
            yield from self._skip_product(response.meta)
        
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É –ø—Ä–æ–¥—É–∫—Ç—É (RU): {response.url} | {e}")
            yield from self._skip_product(response.meta)
            return
    
    def parse_product_error(self, failure):
        """–û–±—Ä–æ–±–∫–∞ –ø–æ–º–∏–ª–æ–∫ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–æ–≤–∞—Ä—É"""
        url = failure.request.url
        reason = failure.value
        product_name = failure.request.meta.get("name_ua", "–ù–∞–∑–≤–∞ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞")
        
        self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–æ–≤–∞—Ä—É: {product_name} ({url}). –ü—Ä–∏—á–∏–Ω–∞: {reason}")
        self.failed_products.append({"url": url, "reason": str(reason), "product_name": product_name})
        
        meta = failure.request.meta
        remaining = meta.get("remaining_products", [])
        category_index = meta.get("category_index")
        
        if remaining:
            next_data = remaining.pop(0)
            next_data["meta"]["remaining_products"] = remaining
            next_data["meta"]["category_index"] = category_index
            
            self.logger.info(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞—é —Ç–æ–≤–∞—Ä. –ó–∞–ª–∏—à–∏–ª–æ—Å—å: {len(remaining)}")
            yield scrapy.Request(
                url=next_data["url"],
                callback=self.parse_product,
                errback=self.parse_product_error,
                meta=next_data["meta"],
                dont_filter=True,
            )
        else:
            self.logger.info(f"‚è≠Ô∏è –í—Å—ñ —Ç–æ–≤–∞—Ä–∏ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó –æ–±—Ä–æ–±–ª–µ–Ω—ñ (–∑ –ø–æ–º–∏–ª–∫–∞–º–∏).")
            next_cat = self._start_next_category(category_index)
            if next_cat:
                yield next_cat
    
    def _skip_product(self, meta):
        """–ü–µ—Ä–µ—Ö—ñ–¥ –¥–æ –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä—É –≤ –ª–∞–Ω—Ü—é–≥—É"""
        remaining = meta.get("remaining_products", [])
        category_index = meta.get("category_index")
        
        if remaining:
            next_data = remaining.pop(0)
            next_data["meta"]["remaining_products"] = remaining
            next_data["meta"]["category_index"] = category_index
            
            self.logger.info(f"‚è≠Ô∏è –ü–µ—Ä–µ—Ö—ñ–¥ –¥–æ –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä—É. –ó–∞–ª–∏—à–∏–ª–æ—Å—å: {len(remaining)}")
            yield scrapy.Request(
                url=next_data["url"],
                callback=self.parse_product,
                errback=self.parse_product_error,
                meta=next_data["meta"],
                dont_filter=True,
            )
        else:
            self.logger.info(f"‚è≠Ô∏è –¢–æ–≤–∞—Ä–∏ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó –∑–∞–∫—ñ–Ω—á–∏–ª–∏—Å—å.")
            next_cat = self._start_next_category(category_index)
            if next_cat:
                yield next_cat
    
    def _start_next_category(self, current_category_index):
        """–î–æ–ø–æ–º—ñ–∂–Ω–∏–π –º–µ—Ç–æ–¥ –¥–ª—è –∑–∞–ø—É—Å–∫—É –Ω–∞—Å—Ç—É–ø–Ω–æ—ó –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó"""
        next_category_index = current_category_index + 1
        if next_category_index < len(self.category_urls):
            next_category_url = self.category_urls[next_category_index]
            self.logger.info(f"üöÄ –°–¢–ê–†–¢ –ù–ê–°–¢–£–ü–ù–û–á –ö–ê–¢–ï–ì–û–†–Ü–á [{next_category_index + 1}/{len(self.category_urls)}]: {next_category_url}")
            return scrapy.Request(
                url=next_category_url,
                callback=self.parse_category,
                meta={
                    "category_url": next_category_url,
                    "category_index": next_category_index,
                    "page_number": 1,
                },
                dont_filter=True,
            )
        else:
            self.logger.info(f"üéâüéâüéâ –í–°–Ü –ö–ê–¢–ï–ì–û–†–Ü–á –¢–ê –ü–†–û–î–£–ö–¢–ò –û–ë–†–û–ë–õ–ï–ù–Ü üéâüéâüéâ")
            return None
    
    def _extract_image_from_srcset(self, response):
        """–í–∏—Ç—è–≥—É—î –Ω–∞–π–±—ñ–ª—å—à–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑ srcset"""
        srcset = response.css("img[alt*='—Ñ–æ—Ç–æ']::attr(srcset)").get()
        
        if srcset:
            urls = re.findall(r'(https?://[^\s]+)\s+\d+w', srcset)
            if urls:
                return urls[-1]
        
        # Fallback –Ω–∞ src
        image_url = response.css("img[alt*='—Ñ–æ—Ç–æ']::attr(src)").get()
        if not image_url:
            image_url = response.css("img[src*='storage']::attr(src)").get()
        
        if image_url and not image_url.startswith('http'):
            image_url = response.urljoin(image_url)
        
        return image_url or ""
    
    def _extract_specifications_eserver(self, response):
        """–ï–∫—Å—Ç—Ä–∞–∫—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∑ —Ç–∞–±–ª–∏—Ü—ñ e-server"""
        specs = []
        
        spec_container = response.css("div.bg-white")
        if not spec_container:
            self.logger.warning(f"‚ö†Ô∏è –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫: {response.url}")
            return specs
        
        spec_rows = spec_container.css("div.flex.justify-between.mx-3")
        
        for row in spec_rows:
            name_element = row.css("div.font-semibold::text").get()
            name = name_element.strip() if name_element else ""
            
            # –í–∏—Ç—è–≥—É—î–º–æ –í–°–Ü —Ç–µ–∫—Å—Ç–æ–≤—ñ –≤—É–∑–ª–∏ –∑—ñ –∑–Ω–∞—á–µ–Ω–Ω—è (–≤–∫–ª—é—á–∞—é—á–∏ –±–∞–≥–∞—Ç–æ—Ä—è–¥–∫–æ–≤—ñ)
            value_elements = row.css("div.text-right::text, div.whitespace-pre-line::text").getall()
            if not value_elements:
                value_elements = row.css("div.font-medium::text").getall()
            
            # –û–±'—î–¥–Ω—É—î–º–æ –≤—Å—ñ —Ç–µ–∫—Å—Ç–æ–≤—ñ –≤—É–∑–ª–∏, –∑–∞–º—ñ–Ω—é—é—á–∏ –ø–µ—Ä–µ–Ω–æ—Å–∏ –Ω–∞ <br> –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ñ–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è
            value = "<br>".join([v.strip() for v in value_elements if v.strip()])
            
            if name and value:
                specs.append({
                    "name": name,
                    "unit": "",
                    "value": value,
                })
        
        return specs
    
    def _extract_description_from_html(self, response):
        """–ï–∫—Å—Ç—Ä–∞–∫—Ç —Ç–µ–∫—Å—Ç—É –æ–ø–∏—Å—É –∑ HTML"""
        description_container = response.css("div.product_pg-dsc__h3fai")
        
        if not description_container:
            return ""
        
        paragraphs = description_container.css("p::text").getall()
        
        if paragraphs:
            return "\n".join([p.strip() for p in paragraphs if p.strip()])
        
        all_text = description_container.css("::text").getall()
        return " ".join([t.strip() for t in all_text if t.strip()])
    
    def _load_keywords_mapping_eserver(self):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –º–∞–ø–ø—ñ–Ω–≥ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤ –¥–ª—è eserver –∑ CSV"""
        import csv
        mapping = {}
        csv_path = Path(r"C:\FullStack\Scrapy\data\eserver\eserver_keywords.csv")
        
        if not csv_path.exists():
            self.logger.warning("eserver_keywords.csv not found")
            return mapping
        
        try:
            with open(csv_path, encoding="utf-8-sig") as f:
                reader = csv.DictReader(f, delimiter=";")
                for row in reader:
                    subdivision_id = row["–Ü–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ç–æ—Ä_–ø—ñ–¥—Ä–æ–∑–¥—ñ–ª—É"].strip()
                    mapping[subdivision_id] = {
                        "keywords_ru": [w.strip() for w in row["keywords_ru"].strip('"').split(",") if w.strip()],
                        "keywords_ua": [w.strip() for w in row["keywords_ua"].strip('"').split(",") if w.strip()],
                        "characteristics_ru": [w.strip() for w in row["characteristics_ru"].strip('"').split(",") if w.strip()],
                        "characteristics_ua": [w.strip() for w in row["characteristics_ua"].strip('"').split(",") if w.strip()],
                    }
            self.logger.info(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(mapping)} –ø—ñ–¥—Ä–æ–∑–¥—ñ–ª—ñ–≤ –∑ –∫–ª—é—á–æ–≤–∏–º–∏ —Å–ª–æ–≤–∞–º–∏ –¥–ª—è eserver")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è eserver_keywords.csv: {e}")
        
        return mapping
    
    def _generate_search_terms(self, title: str, subdivision_id: str = "", lang: str = "ua") -> str:
        """–ì–µ–Ω–µ—Ä—É—î –ø–æ—à—É–∫–æ–≤—ñ —Ç–µ—Ä–º—ñ–Ω–∏ –∑ –Ω–∞–∑–≤–∏ —Ç–æ–≤–∞—Ä—É —Ç–∞ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤"""
        if not title:
            return ""
        
        components = self._extract_model_components(title, lang)
        
        # –î–æ–¥–∞—î–º–æ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –∑ CSV (–ë–õ–û–ö 2)
        if subdivision_id and subdivision_id in self.keywords_mapping:
            keywords_key = f"keywords_{lang}"
            characteristics_key = f"characteristics_{lang}"
            
            category_keywords = self.keywords_mapping[subdivision_id].get(keywords_key, [])
            characteristics = self.keywords_mapping[subdivision_id].get(characteristics_key, [])
            
            # –û–±'—î–¥–Ω—É—î–º–æ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ —Ç–∞ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
            all_keywords = category_keywords + characteristics
            
            # –û–±–∏—Ä–∞—î–º–æ –º–∞–∫—Å–∏–º—É–º 12 –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤
            components.extend(all_keywords[:12])
        
        # –í–∏–¥–∞–ª—è—î–º–æ –¥—É–±–ª—ñ–∫–∞—Ç–∏
        seen = set()
        unique_terms = []
        for term in components:
            term_lower = term.lower()
            if term_lower not in seen:
                unique_terms.append(term)
                seen.add(term_lower)
        
        return ", ".join(unique_terms[:20])  # –û–±–º–µ–∂—É—î–º–æ –¥–æ 20 —Ç–µ—Ä–º—ñ–Ω—ñ–≤
