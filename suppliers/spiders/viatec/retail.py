"""
Spider Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ñƒ Ñ€Ğ¾Ğ·Ğ´Ñ€Ñ–Ğ±Ğ½Ğ¸Ñ… Ñ†Ñ–Ğ½ Ğ· viatec.ua (UAH)
Ğ’Ğ¸Ğ³Ñ€ÑƒĞ¶Ğ°Ñ” Ğ´Ğ°Ğ½Ñ– Ğ²: output/viatec_retail.csv

ĞŸĞĞ¡Ğ›Ğ†Ğ”ĞĞ’ĞĞ ĞĞ‘Ğ ĞĞ‘ĞšĞ: ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ñ–Ñ â†’ Ğ²ÑÑ– ÑÑ‚Ğ¾Ñ€Ñ–Ğ½ĞºĞ¸ Ğ¿Ğ°Ğ³Ñ–Ğ½Ğ°Ñ†Ñ–Ñ— â†’ Ğ½Ğ°ÑÑ‚ÑƒĞ¿Ğ½Ğ° ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ñ–Ñ
Ğ¥ĞĞ ĞĞšĞ¢Ğ•Ğ Ğ˜Ğ¡Ğ¢Ğ˜ĞšĞ˜: Ğ¿Ğ°Ñ€ÑÑÑ‚ÑŒÑÑ Ğ£ĞšĞ ĞĞ‡ĞĞ¡Ğ¬ĞšĞĞ® (UA) Ğ¼Ğ¾Ğ²Ğ¾Ñ
"""
import scrapy
import csv
from pathlib import Path
from suppliers.spiders.base import ViatecBaseSpider, BaseRetailSpider


class ViatecRetailSpider(ViatecBaseSpider, BaseRetailSpider):
    name = "viatec_retail"
    supplier_id = "viatec"
    output_filename = "viatec_retail.csv"
    
    custom_settings = {
        **ViatecBaseSpider.custom_settings,
        "ITEM_PIPELINES": {
            "suppliers.pipelines.SuppliersPipeline": 300,
        }
    }
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.category_mapping = self._load_category_mapping()
        self.category_urls = list(self.category_mapping.keys())
        self.current_category_index = 0
    
    def _load_category_mapping(self):
        """Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ÑƒÑ” Ğ¼Ğ°Ğ¿Ğ¿Ñ–Ğ½Ğ³ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ñ–Ğ¹ Ğ· CSV"""
        mapping = {}
        csv_path = Path(r"C:\FullStack\Scrapy\data\viatec\viatec_category_retail.csv")
        
        try:
            with open(csv_path, encoding="utf-8-sig") as f:
                reader = csv.DictReader(f, delimiter=";")
                for row in reader:
                    url = row["Ğ›Ğ¸Ğ½Ğº ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ñ‰Ğ¸ĞºĞ°"].strip().strip('"')
                    
                    if not url or url == "" or not url.startswith("http"):
                        continue
                    
                    mapping[url] = {
                        "category_ru": row["ĞšĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ñ Ğ½Ğ° Ğ¼Ğ¾ĞµĞ¼ ÑĞ°Ğ¹Ñ‚Ğµ_RU"],
                        "category_ua": row["ĞšĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ñ Ğ½Ğ° Ğ¼Ğ¾ĞµĞ¼ ÑĞ°Ğ¹Ñ‚Ğµ_UA"],
                        "group_number": row.get("ĞĞ¾Ğ¼ĞµÑ€_Ğ³Ñ€ÑƒĞ¿Ğ¸", ""),
                        "subdivision_id": row.get("Ğ†Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ñ–ĞºĞ°Ñ‚Ğ¾Ñ€_Ğ¿Ñ–Ğ´Ñ€Ğ¾Ğ·Ğ´Ñ–Ğ»Ñƒ", ""),
                        "subdivision_link": row.get("ĞŸĞ¾ÑĞ¸Ğ»Ğ°Ğ½Ğ½Ñ_Ğ¿Ñ–Ğ´Ñ€Ğ¾Ğ·Ğ´Ñ–Ğ»Ñƒ", ""),
                    }
            self.logger.info(f"âœ… Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ¾ {len(mapping)} ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ñ–Ğ¹")
        except Exception as e:
            self.logger.error(f"âŒ ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ½Ñ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ñ–Ğ¹: {e}")
        
        return mapping
    
    def start_requests(self):
        """Ğ¡Ñ‚Ğ°Ñ€Ñ‚ÑƒÑ”Ğ¼Ğ¾ Ğ· Ğ¿ĞµÑ€ÑˆĞ¾Ñ— ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ñ–Ñ—"""
        if self.category_urls:
            first_category_url = self.category_urls[0]
            self.logger.info(f"ğŸš€ Ğ¡Ğ¢ĞĞ Ğ¢ ĞŸĞĞ Ğ¡Ğ˜ĞĞ“Ğ£. ĞŸĞµÑ€ÑˆĞ° ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ñ–Ñ [1/{len(self.category_urls)}]: {first_category_url}")
            yield scrapy.Request(
                url=first_category_url,
                callback=self.parse_category,
                meta={
                    "category_url": first_category_url,
                    "category_index": 0,
                    "page_number": 1,
                },
                dont_filter=True,
            )
    
    def parse_category(self, response):
        """ĞŸĞ°Ñ€ÑĞ¸Ğ¼Ğ¾ ÑĞ¿Ğ¸ÑĞ¾Ğº Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñ–Ğ² Ñƒ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ñ–Ñ— Ñ‚Ğ° ÑÑ‚Ğ¾Ñ€Ñ–Ğ½ĞºĞ¸ Ğ¿Ğ°Ğ³Ñ–Ğ½Ğ°Ñ†Ñ–Ñ—"""
        category_url = response.meta["category_url"]
        category_index = response.meta["category_index"]
        page_number = response.meta.get("page_number", 1)
        category_info = self.category_mapping.get(category_url, {})
        
        self.logger.info(f"ğŸ“‚ ĞĞ±Ñ€Ğ¾Ğ±Ğ»ÑÑ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ñ–Ñ [{category_index + 1}/{len(self.category_urls)}] ÑÑ‚Ğ¾Ñ€Ñ–Ğ½ĞºĞ° {page_number}: {category_url}")
        
        product_links = response.css("a[href*='/product/']::attr(href)").getall()
        
        if not product_links:
            self.logger.warning(f"âš ï¸ ĞĞµ Ğ·Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñ–Ğ² Ğ½Ğ° ÑÑ‚Ğ¾Ñ€Ñ–Ğ½Ñ†Ñ–: {response.url}")
        else:
            self.logger.info(f"ğŸ“¦ Ğ—Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñ–Ğ² Ğ½Ğ° ÑÑ‚Ğ¾Ñ€Ñ–Ğ½Ñ†Ñ–: {len(product_links)}")
            for link in product_links:
                product_url = response.urljoin(link)
                normalized_url = product_url.replace("/ru/", "/")
                
                if normalized_url not in self.processed_products:
                    self.products_from_pagination.append({
                        "url": normalized_url,
                        "meta": {
                            "category_url": category_url,
                            "category_ru": category_info.get("category_ru", ""),
                            "category_ua": category_info.get("category_ua", ""),
                            "group_number": category_info.get("group_number", ""),
                            "subdivision_id": category_info.get("subdivision_id", ""),
                            "subdivision_link": category_info.get("subdivision_link", ""),
                        },
                    })
                    self.processed_products.add(normalized_url)
        
        next_page_link = response.css("a.paggination__next::attr(href)").get()
        if not next_page_link:
            all_pages = response.css("a.paggination__page::attr(href)").getall()
            active_page_nodes = response.css("a.paggination__page--active")
            if all_pages and active_page_nodes:
                try:
                    active_page_text = active_page_nodes[0].css("::text").get()
                    all_page_texts = [a.css("::text").get() for a in response.css("a.paggination__page")]
                    current_idx = all_page_texts.index(active_page_text)
                    
                    if current_idx >= 0 and current_idx + 1 < len(all_pages):
                        next_page_link = all_pages[current_idx + 1]
                except (ValueError, IndexError):
                    pass
        
        if next_page_link:
            self.logger.info(f"ğŸ“„ ĞŸĞµÑ€ĞµÑ…Ñ–Ğ´ Ğ½Ğ° Ğ½Ğ°ÑÑ‚ÑƒĞ¿Ğ½Ñƒ ÑÑ‚Ğ¾Ñ€Ñ–Ğ½ĞºÑƒ Ğ¿Ğ°Ğ³Ñ–Ğ½Ğ°Ñ†Ñ–Ñ— ({page_number + 1}): {next_page_link}")
            yield response.follow(
                next_page_link,
                callback=self.parse_category,
                meta={
                    "category_url": category_url,
                    "category_index": category_index,
                    "page_number": page_number + 1,
                },
                dont_filter=True,
            )
        else:
            self.logger.info(f"âœ… ĞŸĞĞ“Ğ†ĞĞĞ¦Ğ†Ğ¯ Ğ—ĞĞ’Ğ•Ğ Ğ¨Ğ•ĞĞ [{category_index + 1}/{len(self.category_urls)}]: Ğ½Ğ°ĞºĞ¾Ğ¿Ğ¸Ñ‡ĞµĞ½Ğ¾ {len(self.products_from_pagination)} Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñ–Ğ²")
            
            if self.products_from_pagination:
                product_data = self.products_from_pagination.pop(0)
                product_data["meta"]["remaining_products"] = self.products_from_pagination
                product_data["meta"]["category_index"] = category_index
                
                self.logger.info(f"ğŸ”— Ğ—ĞĞŸĞ£Ğ¡Ğš Ğ»Ğ°Ğ½Ñ†ÑĞ³Ğ° Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ñ–Ğ². ĞŸĞµÑ€ÑˆĞ¸Ğ¹: {product_data['url']}. Ğ—Ğ°Ğ»Ğ¸ÑˆĞ¸Ğ»Ğ¾ÑÑŒ: {len(self.products_from_pagination)}")
                
                yield scrapy.Request(
                    url=product_data["url"],
                    callback=self.parse_product,
                    errback=self.parse_product_error,
                    meta=product_data["meta"],
                    dont_filter=True,
                )
            else:
                self.logger.warning(f"âš ï¸ Ğ£ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ñ–Ñ— {category_url} Ğ½Ğµ Ğ·Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñ–Ğ². ĞŸĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¶Ñƒ Ğ´Ğ¾ Ğ½Ğ°ÑÑ‚ÑƒĞ¿Ğ½Ğ¾Ñ—.")
                yield self._start_next_category(category_index)
            
            self.products_from_pagination = []
    
    def parse_product(self, response):
        """ĞŸĞ°Ñ€ÑĞ¸Ğ¼Ğ¾ ÑÑ‚Ğ¾Ñ€Ñ–Ğ½ĞºÑƒ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñƒ (ÑƒĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ° Ğ²ĞµÑ€ÑÑ–Ñ) - ĞĞĞ—Ğ’Ğ, ĞĞŸĞ˜Ğ¡, Ğ¥ĞĞ ĞĞšĞ¢Ğ•Ğ Ğ˜Ğ¡Ğ¢Ğ˜ĞšĞ˜"""
        try:
            self.logger.info(f"ğŸ”— ĞŸĞ°Ñ€ÑĞ¸Ğ¼Ğ¾ Ñ‚Ğ¾Ğ²Ğ°Ñ€ (UA): {response.url}")
            
            name_ua = response.css("h1::text").get()
            name_ua = name_ua.strip() if name_ua else ""
            
            description_ua = self._extract_description_with_br(response)
            specs_list_ua = self._extract_specifications(response)
            
            self.logger.info(f"ğŸ“ Ğ¥Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº (UA) Ğ·Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾: {len(specs_list_ua)} ÑˆÑ‚.")
            
            ru_url = self._convert_to_ru_url(response.url)
            
            yield scrapy.Request(
                url=ru_url,
                callback=self.parse_product_ru,
                errback=self.parse_product_error,
                meta={
                    **response.meta,
                    "name_ua": name_ua,
                    "description_ua": description_ua,
                    "specifications_list": specs_list_ua,
                    "original_url": response.url,
                },
                dont_filter=True,
            )
        except Exception as e:
            self.logger.error(f"âŒ ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ñƒ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ñƒ (UA): {response.url} | {e}")
            yield from self._skip_product(response.meta)
            return
    
    def parse_product_ru(self, response):
        """ĞŸĞ°Ñ€ÑĞ¸Ğ¼Ğ¾ ÑÑ‚Ğ¾Ñ€Ñ–Ğ½ĞºÑƒ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñƒ (Ñ€Ğ¾ÑÑ–Ğ¹ÑÑŒĞºĞ° Ğ²ĞµÑ€ÑÑ–Ñ) Ñ‚Ğ° Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ²Ğ¶ÑƒÑ”Ğ¼Ğ¾ Ğ»Ğ°Ğ½Ñ†ÑĞ³"""
        try:
            self.logger.info(f"ğŸ”— ĞŸĞ°Ñ€ÑĞ¸Ğ¼Ğ¾ Ñ‚Ğ¾Ğ²Ğ°Ñ€ (RU): {response.url}")
            
            name_ru = response.css("h1::text").get()
            name_ru = name_ru.strip() if name_ru else ""
            
            description_ru = self._extract_description_with_br(response)
            
            name_ua = response.meta.get("name_ua", "")
            description_ua = response.meta.get("description_ua", "")
            specs_list = response.meta.get("specifications_list", [])
            
            code = ""
            price_raw = response.css("div.card-header__card-price-new::text").get()
            price_raw = price_raw.strip().replace("&nbsp;", "").replace(" ", "") if price_raw else ""
            price = self._clean_price(price_raw) if price_raw else ""
            
            self.logger.info(f"ğŸ“ ĞĞ¿Ğ¸Ñ RU: {len(description_ru)} ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ–Ğ²")
            self.logger.info(f"ğŸ“ ĞĞ¿Ğ¸Ñ UA: {len(description_ua)} ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ–Ğ²")
            
            images = response.css("img.card-header__card-images-image::attr(src)").getall()
            image_url = response.urljoin(images[0]) if images else ""
            
            availability_raw_text = response.css("div.card-header__card-status-badge::text").get()
            availability_status = self._normalize_availability(availability_raw_text)
            quantity = self._extract_quantity(availability_raw_text)
            
            manufacturer = self._extract_manufacturer(name_ru)
            
            group_number = response.meta.get("group_number", "")
            search_terms_ru = self._generate_search_terms(name_ru, group_number, lang="ru")
            search_terms_ua = self._generate_search_terms(name_ua, group_number, lang="ua")
            
            item = {
                "ĞšĞ¾Ğ´_Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñƒ": code,
                "ĞĞ°Ğ·Ğ²Ğ°_Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ñ–Ñ—": name_ru,
                "ĞĞ°Ğ·Ğ²Ğ°_Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ñ–Ñ—_ÑƒĞºÑ€": name_ua,
                "ĞŸĞ¾ÑˆÑƒĞºĞ¾Ğ²Ñ–_Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ğ¸": search_terms_ru,
                "ĞŸĞ¾ÑˆÑƒĞºĞ¾Ğ²Ñ–_Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ğ¸_ÑƒĞºÑ€": search_terms_ua,
                "ĞĞ¿Ğ¸Ñ": description_ru,
                "ĞĞ¿Ğ¸Ñ_ÑƒĞºÑ€": description_ua,
                "Ğ¢Ğ¸Ğ¿_Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñƒ": "r",
                "Ğ¦Ñ–Ğ½Ğ°": price,
                "Ğ’Ğ°Ğ»ÑÑ‚Ğ°": self.currency,
                "ĞĞ´Ğ¸Ğ½Ğ¸Ñ†Ñ_Ğ²Ğ¸Ğ¼Ñ–Ñ€Ñƒ": "ÑˆÑ‚.",
                "ĞŸĞ¾ÑĞ¸Ğ»Ğ°Ğ½Ğ½Ñ_Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ½Ñ": image_url,
                "ĞĞ°ÑĞ²Ğ½Ñ–ÑÑ‚ÑŒ": availability_status,
                "ĞšÑ–Ğ»ÑŒĞºÑ–ÑÑ‚ÑŒ": quantity,
                "ĞĞ°Ğ·Ğ²Ğ°_Ğ³Ñ€ÑƒĞ¿Ğ¸": response.meta.get("category_ru", ""),
                "ĞĞ°Ğ·Ğ²Ğ°_Ğ³Ñ€ÑƒĞ¿Ğ¸_ÑƒĞºÑ€": response.meta.get("category_ua", ""),
                "ĞĞ¾Ğ¼ĞµÑ€_Ğ³Ñ€ÑƒĞ¿Ğ¸": response.meta.get("group_number", ""),
                "Ğ†Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ñ–ĞºĞ°Ñ‚Ğ¾Ñ€_Ğ¿Ñ–Ğ´Ñ€Ğ¾Ğ·Ğ´Ñ–Ğ»Ñƒ": response.meta.get("subdivision_id", ""),
                "ĞŸĞ¾ÑĞ¸Ğ»Ğ°Ğ½Ğ½Ñ_Ğ¿Ñ–Ğ´Ñ€Ğ¾Ğ·Ğ´Ñ–Ğ»Ñƒ": response.meta.get("subdivision_link", ""),
                "Ğ’Ğ¸Ñ€Ğ¾Ğ±Ğ½Ğ¸Ğº": manufacturer,
                "ĞšÑ€Ğ°Ñ—Ğ½Ğ°_Ğ²Ğ¸Ñ€Ğ¾Ğ±Ğ½Ğ¸Ğº": "",
                "price_type": self.price_type,
                "supplier_id": self.supplier_id,
                "output_file": self.output_filename,
                "ĞŸÑ€Ğ¾Ğ´ÑƒĞºÑ‚_Ğ½Ğ°_ÑĞ°Ğ¹Ñ‚Ñ–": response.meta.get("original_url", response.url),
                "specifications_list": specs_list,
            }
            
            self.logger.info(f"âœ… YIELD: {item['ĞĞ°Ğ·Ğ²Ğ°_Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ñ–Ñ—']} | Ğ¦Ñ–Ğ½Ğ°: {item['Ğ¦Ñ–Ğ½Ğ°']} | Ğ¥Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº: {len(specs_list)}")
            yield item
            
            yield from self._skip_product(response.meta)
        
        except Exception as e:
            self.logger.error(f"âŒ ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ñƒ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ñƒ (RU): {response.url} | {e}")
            yield from self._skip_product(response.meta)
            return
    
    def parse_product_error(self, failure):
        url = failure.request.url
        reason = failure.value
        product_name = failure.request.meta.get("name_ru", "ĞĞ°Ğ·Ğ²Ğ° Ğ½Ğµ Ğ·Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°")
        
        self.logger.error(f"âŒ ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ½Ñ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñƒ: {product_name} ({url}). ĞŸÑ€Ğ¸Ñ‡Ğ¸Ğ½Ğ°: {reason}")
        self.failed_products.append({"url": url, "reason": str(reason), "product_name": product_name})
        
        meta = failure.request.meta
        remaining = meta.get("remaining_products", [])
        category_index = meta.get("category_index")
        
        if remaining:
            next_data = remaining.pop(0)
            next_data["meta"]["remaining_products"] = remaining
            next_data["meta"]["category_index"] = category_index
            
            self.logger.info(f"â­ï¸ ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°Ñ Ñ‚Ğ¾Ğ²Ğ°Ñ€. Ğ—Ğ°Ğ»Ğ¸ÑˆĞ¸Ğ»Ğ¾ÑÑŒ: {len(remaining)}")
            yield scrapy.Request(
                url=next_data["url"],
                callback=self.parse_product,
                errback=self.parse_product_error,
                meta=next_data["meta"],
                dont_filter=True,
            )
        else:
            self.logger.info(f"â­ï¸ Ğ’ÑÑ– Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ñ–Ñ— Ğ¾Ğ±Ñ€Ğ¾Ğ±Ğ»ĞµĞ½Ñ– (Ğ· Ğ¿Ğ¾Ğ¼Ğ¸Ğ»ĞºĞ°Ğ¼Ğ¸).")
            next_cat = self._start_next_category(category_index)
            if next_cat:
                yield next_cat
    
    def _skip_product(self, meta):
        remaining = meta.get("remaining_products", [])
        category_index = meta.get("category_index")
        
        if remaining:
            next_data = remaining.pop(0)
            next_data["meta"]["remaining_products"] = remaining
            next_data["meta"]["category_index"] = category_index
            
            self.logger.info(f"â­ï¸ ĞŸĞµÑ€ĞµÑ…Ñ–Ğ´ Ğ´Ğ¾ Ğ½Ğ°ÑÑ‚ÑƒĞ¿Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñƒ. Ğ—Ğ°Ğ»Ğ¸ÑˆĞ¸Ğ»Ğ¾ÑÑŒ: {len(remaining)}")
            yield scrapy.Request(
                url=next_data["url"],
                callback=self.parse_product,
                errback=self.parse_product_error,
                meta=next_data["meta"],
                dont_filter=True,
            )
        else:
            self.logger.info(f"â­ï¸ Ğ¢Ğ¾Ğ²Ğ°Ñ€Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ñ–Ñ— Ğ·Ğ°ĞºÑ–Ğ½Ñ‡Ğ¸Ğ»Ğ¸ÑÑŒ.")
            next_cat = self._start_next_category(category_index)
            if next_cat:
                yield next_cat
    
    def _start_next_category(self, current_category_index):
        """Ğ”Ğ¾Ğ¿Ğ¾Ğ¼Ñ–Ğ¶Ğ½Ğ¸Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿ÑƒÑĞºÑƒ Ğ½Ğ°ÑÑ‚ÑƒĞ¿Ğ½Ğ¾Ñ— ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ñ–Ñ—"""
        next_category_index = current_category_index + 1
        if next_category_index < len(self.category_urls):
            next_category_url = self.category_urls[next_category_index]
            self.logger.info(f"ğŸš€ Ğ¡Ğ¢ĞĞ Ğ¢ ĞĞĞ¡Ğ¢Ğ£ĞŸĞĞĞ‡ ĞšĞĞ¢Ğ•Ğ“ĞĞ Ğ†Ğ‡ [{next_category_index + 1}/{len(self.category_urls)}]: {next_category_url}")
            return scrapy.Request(
                url=next_category_url,
                callback=self.parse_category,
                meta={
                    "category_url": next_category_url,
                    "category_index": next_category_index,
                    "page_number": 1,
                },
                dont_filter=True,
            )
        else:
            self.logger.info(f"ğŸ‰ğŸ‰ğŸ‰ Ğ’Ğ¡Ğ† ĞšĞĞ¢Ğ•Ğ“ĞĞ Ğ†Ğ‡ Ğ¢Ğ ĞŸĞ ĞĞ”Ğ£ĞšĞ¢Ğ˜ ĞĞ‘Ğ ĞĞ‘Ğ›Ğ•ĞĞ† ğŸ‰ğŸ‰ğŸ‰")
            return None
